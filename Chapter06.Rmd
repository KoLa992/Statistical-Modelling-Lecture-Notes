---
title: "Estimating Parameters of Probability Distributions"
author: "László Kovács"
date: "23/02/2025"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Method of Moments

General concept is what we have used for normal distribution in case of the Tesla price change data in <a href="Chapter03.html" target="_blank">Chapter 3</a>. For a normal distribution $Y \sim N(\mu, \sigma)$ we say that the sample moments are the best estimators for the theoretical moments $\hat{\mu}=\bar{y}$ and $\hat{\sigma}^2=s^2$

### 1.1. Bernoulli and Poisson Distributions

If we have $X \sim I(p)$ and $Y ~ P(\lambda)$, then we know that $E(X)=p$ and $E(Y)=\lambda$.

We have an IID sample of size $n$ from both distributions with the sample observations defined as $y=(y_1,y_2,...,y_n)$ and  $x=(x_1,x_2,...,x_n)$ respectively. The sample means $\bar{x}$ and $\bar{y}$ are the "*sample versions*" of the expected values of two distributions. So we can simply say that the sample mean of $x$ (denoted as $\hat{x}$) is an estimator of $p$ (denoted as $\hat{p}$): $$\bar{x}=\hat{p}$$

And of course, we can also say that the sample mean of $y$ is an estimator of $p$:$$\bar{y}=\hat{\lambda}$$

Read the data in the <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/insurance_examples.xlsx" target="_blank">insurance_examples.xlsx</a> file.

```{r}
library(readxl)
is_claim <- read_excel("insurance_examples.xlsx", sheet = "CarInsurance_IsClaim")
num_claims <- read_excel("insurance_examples.xlsx", sheet = "CarInsurance_NumberOfClaims")
str(is_claim)
str(num_claims)
```

Do the estimation with method of moments.

```{r}
p_estim <- mean(is_claim$IsClaim)
lambda_estim <- mean(num_claims$NumClaims)
p_estim
lambda_estim
```

See the goodness of fit for the Bernoulli Distribution.

```{r}
observed_frequencies <- table(is_claim$IsClaim)
expected_frequencies <- c(p_estim*nrow(is_claim), (1-p_estim)*nrow(is_claim))
names(expected_frequencies) <- c("1", "0")

observed_frequencies
expected_frequencies
```

See the goodness of fit for the Poisson Distribution.

```{r}
observed_frequencies <- table(num_claims$NumClaims)
expected_frequencies <- round(dpois(x = unique(num_claims$NumClaims), lambda = lambda_estim) * nrow(num_claims), 0)
names(expected_frequencies) <- names(observed_frequencies)

observed_frequencies
expected_frequencies
```

### 1.2. Binomial Distribution

A more advanced case for Binomial distribution. Let's say that we have an insurance portfolio for car fleets of small businesses with a maximum number of $n$ cars. The *CarInsurance_Fleet* sheet of the *insurance_examples.xlsx* file contains $150$ insured car fleets and the `NumClaims_Fleet` variable shows the number of cars with accidents reported in a given year for each fleet.

```{r}
carinsurance_fleet <- read_excel("insurance_examples.xlsx", sheet = "CarInsurance_Fleet")
str(carinsurance_fleet)
```

If each car has a probability of $p$ to have an accident in a year, then we can say that the number of claims reported in a fleet (let's denote this variable as $X$) has a Binomial Distribution: $X \sim B(n,p)$

This time, we have two parameters to estimate: $n$ and $p$. Here we need two equations to determine the two parameters. We can use the definitions for the expected value and variance: $E(X)=np$ and $Var(X)=np(1-p)$. These theoretical definitions can also be assumed to hold in the observed IID sample too. So, the two equations to solve for $n$ and $p$ are: $$\bar{x}=\hat{n}\hat{p}$$

And of course: $$s^2=\hat{n}\hat{p}(1-\hat{p})$$

We use the corrected sample variance, $s^2$ here of course, as we should not underestimate the second moment.

To solve this system of two equations, we should first substitute the first equation into the second one: $$s^2=\bar{x} \times (1-\hat{p})$$

This, we can solve for $\hat{p}$: $$\hat{p}=1-\frac{s^2}{\bar{x}}$$

We can try this formula rather easily in R.

```{r}
p_binom_estim <- 1-var(carinsurance_fleet$NumClaims_Fleet)/mean(carinsurance_fleet$NumClaims_Fleet)
p_binom_estim
```

Great, we have that $\hat{p}=0.016$ for our data.

With $\hat{p}$, it's easy to obtain $\hat{n}$ from the first equation: $$\hat{n}=\frac{\bar{x}}{\hat{p}}$$

In R, we just simply follow this formula above as well.

```{r}
n_binom_estim <- mean(carinsurance_fleet$NumClaims_Fleet)/p_binom_estim
n_binom_estim
```

As $n$ most be an integer, we should round the result for $\hat{n}$. if we aim to be cautious, then we take the highest possible value on the maximum number of the claims, so we round $\hat{n}$ to $10$.

```{r}
n_binom_estim <- ceiling(n_binom_estim)
n_binom_estim
```

See the goodness of fit for the Binomial Distribution.

```{r}
observed_frequencies <- table(carinsurance_fleet$NumClaims_Fleet)
expected_frequencies <- round(dbinom(x = unique(carinsurance_fleet$NumClaims_Fleet),
                                     size = n_binom_estim, prob = p_binom_estim) *
                                nrow(num_claims), 0)
names(expected_frequencies) <- names(observed_frequencies)

observed_frequencies
expected_frequencies
```

### 1.3. Exponential Distribution

In case of continuous distributions, let's see the Exponential Distribution as an example.

Examine the chemotherapy survival times found in the <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/CancerSurvival.xlsx\" target="_blank">CancerSurvival.xlsx</a> file again.

```{r}
Surv <- read_excel("CancerSurvival.xlsx")
str(Surv)
```

If we remember from the histogram it has quite a long right tail, so the Exponential distribution would be a good fit.

```{r}
hist(Surv$SurvMonth)
```

If we have $Y \sim Exp(\lambda)$, then the expected value is $E(X)=\frac{1}{\lambda}$, so if this is true in the sample as well, then the equation to solve is: $$\bar{x}=\frac{1}{\hat{\lambda}}$$

This solution is just a reciprocal, so: $$\hat{\lambda}=\frac{1}{\bar{x}}=\frac{n}{\sum_{i=1}^{n}{x_i}}$$

So, as we did in <a href="Chapter03.html" target="_blank">Chapter 3</a>, let's take the reciprocal of the sample mean, and we'll have the same $Exp(0.00442)$ distribution again for the cancer survival times.

```{r}
1/mean(Surv$SurvMonth)
```

**Issue**: why not $\hat{\lambda}=\frac{1}{s}$?

### 1.4. Special Cases: Using Mean and Median

And an interesting trick: same principle as for the binomial case, but with the mean and median of the lognormal distribution! <a href="https://github.com/tamas-ferenci/omsz-kierkezesi-ido-percentilis?tab=readme-ov-file#az-orsz%C3%A1gos-ment%C5%91szolg%C3%A1lat-ki%C3%A9rkez%C3%A9si-statisztik%C3%A1inak-vizsg%C3%A1lata-a-90percentilis-becsl%C3%A9se" target="_blank">This example</a> should be translated and in included here!

## 2. Maximum Likelihood Estimation

General principle. Let's **find those parameters** for a given probability distribution that **maximize the probability of all our observed data occurring in an IID sample** (i.e. random sample with replacement) from that given distribution. So, if the elements of the observed sample of size $n$ are denoted as $y=(y_1,y_2,...,y_n)$, and the parameters of the probability distribution we are looking for denoted as $\theta$, then we are looking for the $\theta$ that maximizes the probability of $y$ occurring from $n$ random draws from the examined distribution.<br>
A mathematical formulation for this task is as follows: $$P(y|\theta) \rightarrow \max_{\theta}$$

Here, the $P(y|\theta)$ probability is called as the **likelihood of the sample**. This is a **conditional probability** of the sample occurring and the condition is that we give some arbitrary values for the $\theta$ values. **We are looking for those $\theta$ conditions that maximize our $y$ sample occurring.**<br>
To calculate this, we can **use the $f(x,\theta)$ mass/density functions of the discrete/continuous distributions**, as they **show the probability of an arbitrary number $x$ occurring in a random draw from the probability distribution with $\theta$ parameters**. So if we say that every $y_1,y_2,...,y_n$ observations of our sample are from the same probability distribution with the same mass/density function denoted as $f$, then this function can be used to give the **probability of the a specific $y_i$ sample observations occurring in a random draw as $f(y_i, \theta)$.**

So, the **probability of each $y_i$ observation of our sample occurring for a random draw under some $\theta$ parameter conditions** is given as $f(y_i)=P(y_i|\theta)$ and this is the **likelihood of the $y_i$ observation**. The **likelihood of the whole sample is the probability of all the observed $y_i$ values ($y_1,y_2,...,y_n$) occurring all at once!** This is the **PRODUCT of the $P(y_i|\theta)$ probabilities if the $y_i$ values are independent**, and since **we assume that we work with IID** (independent, identically distributed) **samples, the independence of the $y_i$ values can also be assumed**, because of the first "I" in IID. :)<br>
Therefore, the **likelihood of our whole $y$ sample under the condition of some $\theta$ parameters in an IID sample of size $n$** is given as follows: $$L(\theta)=\prod_{i=1}^{n}{P(y_i|\theta)}=\prod_{i=1}^{n}{f(y_i, \theta)}$$

And we need to **maximize this $L(\theta)$ by varying the $\theta$ conditions**. As this is what is the **direct expression of the maximum likelihood principle**: *find those parameters that maximize the probability of all our observed data occurring in an IID sample*.

To solve this maximization task, we need to take the $\frac{\partial L(\theta)}{\partial\theta}$ derivatives and make them equal to $0$. However, **taking the derivative of a product-function that is $L(\theta)$ is rather painful**. Best to avoid this. :) And we can by **maximizing the log-likelihood function of the sample**, as **on a log-scale a multiplicative function becomes additive!** $$l(\theta)=\ln(L(\theta))=\ln\prod_{i=1}^{n}{f(y_i,\theta)}=\sum_{i=1}^{n}{\ln(f(y_i,\theta)})$$

So, in practice, to **make our jobs easy, we maximize the log-likelihood function in $\theta$:** $$l(\theta) \rightarrow \max_{\theta}$$

And we can simply do this maximization by taking the deriavtive of the log-likelihood according to $\theta$ and solve the equation of this derivative being equal to $0$: $$\frac{\partial l(\theta)}{\partial\theta}=0$$

Now, let's see some **practical examples** for this maximum likelihood estimation process!

### 2.1. Manually for Binomial Distribution

The example of number of cars having accident in a fleet of $n$ cars from *Section 1*.

Parameters to estimate: $n$ and $p$, so in this case: $\theta=(n,p)$

Write the negative log-likelihood function for our sample. Negative log-likelihood is necessary as unfortunately this stupid R can only minimize a function by default, and NOT maximize.

```{r}
# function definition
neg_log_likelihood_binom <- function(theta_binom){
  log_lik_observations <- log(dbinom(x = carinsurance_fleet$NumClaims_Fleet,
                                     size = theta_binom[1],
                                     prob = theta_binom[2])) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_binom(c(8, 0.1))
```

Do the optimization. We can *ignore the possible warnings*, it just had some very bad parameter combinations when trying out different $n$ and $p$ values during the optimization.

```{r warning=FALSE}
binom_ml <- optim(c(8, 0.1), neg_log_likelihood_binom)
binom_ml$par # optimal parameter values
binom_ml$val # minimized negative log-likelihood of the sample
```

So, our results are $\hat{n}_{ML}=8$ and $\hat{p}_{ML}=0.099$. We rounded up $\hat{n}_{ML}$ to the next integer as its value needs to be an integer. It's the same thing we did in *Section 1*.

### 2.2. Manually for Exponential Distribution

Cancel survival data from <a href="Chapter03.html" target="_blank">Chapter 3</a>.

Parameters to estimate: $\lambda$, so in this case: $\theta=\lambda$

Write the negative log-likelihood function for our sample.

```{r}
# function definition
neg_log_likelihood_expon <- function(theta_expon){
  log_lik_observations <- log(dexp(x = Surv$SurvMonth,
                                   rate = theta_expon)) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_expon(0.001)
```

Do the optimization. We can *ignore the possible warnings*, it just had some very bad parameter combinations when trying out different $n$ and $p$ values during the optimization.

```{r warning=FALSE}
expon_ml <- optim(0.001, neg_log_likelihood_expon)
expon_ml$par # optimal parameter value
expon_ml$val # minimized negative log-likelihood of the sample
```

So, our result is $\hat{\lambda}_{ML}=0.00442$. So, we need to take the reciprocal of the mean and NOT the standard deviation when estimating $\lambda$ of an exponential distribution.

### 2.3. Manually for Normal Distribution

For the cancer survival data as well: to see a bad fit for the observed data!

Parameters to estimate: $\mu$ and $\sigma$, so in this case: $\theta=(\mu,\sigma)$

Write the negative log-likelihood function for our sample in case of the normal distribution.

```{r}
# function definition
neg_log_likelihood_norm <- function(theta_norm){
  log_lik_observations <- log(dnorm(x = Surv$SurvMonth,
                                    mean = theta_norm[1],
                                    sd = theta_norm[2])) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_norm(c(250, 250))
```

Do the optimization. We can *ignore the warnings*, it just had some very bad parameter combinations when trying out different $n$ and $p$ values during the optimization.

```{r warning=FALSE}
norm_ml <- optim(c(250, 250), neg_log_likelihood_norm)
norm_ml$par # optimal parameter values
norm_ml$val # minimized negative log-likelihood of the sample
```

So, our results are $\hat{\mu}_{ML}=226.11$ and $\hat{p}_{ML}=271.55$.

See that the $\hat{p}_{ML}$ is actually the **uncorrected sample standard deviation**. Difference is just some rounding error.

```{r}
sqrt(mean((Surv$SurvMonth - mean(Surv$SurvMonth))^2))
```

And **NOT the corrected sample standard deviation**.

```{r}
sd(Surv$SurvMonth)
```

### 2.4. Compare Diferent Distributions Fitted on the Same Sample - Information Criterions

Compare the normal and exponential distributions for the cancer survival data. Graphically and logically, its obvious that the exponenetial is a better fit.

```{r}
library(ggplot2)

ggplot(data = Surv, mapping = aes(x=SurvMonth)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, 
                args = list(mean = norm_ml$par[1], sd = norm_ml$par[2]),
                col = 'red') +
  stat_function(fun = dexp, 
                args = list(rate = expon_ml$par),
                col = 'blue')
  
```

We can see that exponential fits better by having smaller minimized negative log-likelihood value, i.e.: smaller "*error*".

```{r}
expon_ml$value < norm_ml$value
```

However, we need to control for the fact that the normal distribution is more flexible as it has two parameters instead of one. *!Note to self: describe over fitting!*


Hence the Information Criterion ($IC$). As all the $IC$s are **based on the negative log-likelihood, they always needs to be minimized**. So, the smaller $IC$ shows a better distribution fit.

Akaike ($AIC$), with $p$ being the number of estimated parameters: $$2p-2l(\theta)$$ 

It's easy to calculate in R.

```{r}
aic_exp <- 2*1 + 2*expon_ml$value # as the exponential distribution has p=1 parameter
aic_norm <- 2*2 + 2*norm_ml$value # as the normal distribution has p=2 parameters
```

And Bayes - Schwarz ($BIC$ or $SBC$): $$p\ln(n) - 2l(\theta)$$

And implemented in R as follows.

```{r}
bic_exp <- 1*log(nrow(Surv)) + 2*expon_ml$value # as the exponential distribution has p=1 parameter
bic_norm  <- 2*log(nrow(Surv)) + 2*norm_ml$value # as the normal distribution has p=2 parameters
```

When calculating these two values, AIC and BIC show that the exponential is a better fit, despite having one parameter instead of two.

```{r}
aic_exp < aic_norm
bic_exp < bic_norm
```


## 3. Automated Distribution Fitting with the `fitdistrplus` package

Install and load the package to your R environment.

```{r eval=FALSE}
install.packages("fitdistrplus")
library(fitdistrplus)
```
```{r echo=FALSE}
library(fitdistrplus)
```

Do the maximum likelihood fitting for normal and exponential distributions for the cancer survival data.

```{r}
fit_expon <- fitdist(Surv$SurvMonth, "exp", method = "mle")
fit_norm <- fitdist(Surv$SurvMonth, "norm", method = "mle")

summary(fit_expon)
summary(fit_norm)
```

You can see the standard errors, $SE$ for the estimated parameters! See that the $IC$s prefer the exponenetial fit.

And do a graphical comparison of the two fitted distributions on the histogram.

```{r}
denscomp(list(fit_expon, fit_norm))
```

Same result as with the ggplot in *Section 2*.