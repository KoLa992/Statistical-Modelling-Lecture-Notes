---
title: "Probability Distributions"
author: "László Kovács"
date: "23/02/2025"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 1. Normal Distribution and its Density Function
 
Let’s examine the data in the table titled <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/TSLA.xlsx\" target="_blank">TSLA.xlsx</a>, which shows the **daily closing price changes of TESLA stocks in dollars** ($) from May 2019 to May 2020.

Excel tables, if the first value of our data table starts in the *A1* cell and we have only one worksheet, can easily be read into a data frame using the `readxl` package’s `read_excel` function. We did this with the *BP_Flats.xlsx* table in the previous chapter as well.

```{r}
library(readxl)

Tesla <- read_excel("TSLA.xlsx")

str(Tesla)
```

We can see that we have two columns: the date and the price change of the stock on the given day compared to the previous day’s closing price. So, on 07/05/2019, a Tesla stock was worth $8.3\$$ less by the end of the day than it was worth on 06/05/2019. However, on 13th May, it was worth $12.5\$$ less than the previous day of 12th May. Based on the results of the `str` function, we have $N=250$ days of such data, which roughly matches the number of trading days in a year.

Now, based on the first five observed days, I wouldn’t want to be in Elon Musk’s place, as the stock is showing some pretty big losses. But let’s see what the distribution shown in the histogram tells us on how this miracle company performed over the full 1-year period under our review!<br>
Let's **draw the histogram** of the Tesla price changes **such that relative frequencies are on the $y$ axis**! This is done by setting the `freq=FALSE` parameter in the `hist` function.

```{r}
hist(Tesla$TESLA, freq = FALSE)
```

Ok, we can see that most days have a price change close to $0$ and the larger and smaller price changes are rarer and rarer...so the **histogram shows a symmetric, normal distribution, although a bit pointy** (kurtosis is positive).

Now, let's imagine what happens when we **connect the columns of the histogram with a continuous line**...We don't need much fantasy to see that connecting the columns **results in a function similar to the one just below**:

```{r echo=FALSE}
mu = mean(Tesla$TESLA)
szigma = sd(Tesla$TESLA)

x <- seq(mu-3*szigma, mu+3*szigma)
y <- dnorm(x, mean = mu, sd = szigma)

DFRajz <- data.frame(x=x, y=y)

library(ggplot2)
ggplot(DFRajz, aes(x=x, y=y)) + geom_line(linewidth=1)
```

Now, this beauty right here is the **density function of the normal distribution**. Or, to be more precise this is the **density function of a normal distribution with a mean of $\mu=1.8\$$ and the standard deviation of $\sigma=27.19\$$ since this is the mean and standard deviation of the variable containing the Tesla price changes**, based on which we've drawn this density function:

```{r}
mu = mean(Tesla$TESLA)
sigma = sd(Tesla$TESLA)

mu
sigma
```

So, to what we can say is that we see the density function of a $N(1.8,27.19)$ distribution. Therefore, the **general notation** for normal distributions is $N(\mu, \sigma)$. And with the notation $Y_i \sim N(1.8,27.19)$ we say that our $Y_i$ observations (now the Tesla price changes) follow a $N(1.8,27.19)$ distribution.

Why is that? because, the exact shape of the density function for the normal distribution depends on the mean and standard deviation of the data that we want to fit this density function on.

- The **mean** defines where the **maximum of the function** is (since for symmetrical distributions the mean equals the mode)
- The **standard deviation** defines how **pointy or flat** the function is

You can see all of this in the interactive plot below:

<iframe src ="https://kola992.shinyapps.io/normaldensityplot/" height=500px width=800px data-external="1" />

The $f(x)$ formula for the density function with a given $\mu$ and $\sigma$ is the following: $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

Don't get PTSD from Calculus classes: we won't use this formula directly. It's just good to see one time what kind of formula can produce this beautiful bell shape of the density function! :)

### 1.1. Using the Desnity Function

Ok, but "*What have the Romans ever done for us?*". So, we can rightly ask the question: what is this density function good for? First and foremost, if we substitute an $x$ number to the $f(x)$ formula of the density function, we get **the probability for a random number drawn from the variable $Y$ being equal with $x$.** With mathematical notations, this means $f(x)=P(Y_i=x)$.<br>
Ok, technically I am not very accurate here, since the probability of the $Y_i=x$ event would be practically $0$. Since a variable with normal distribution can take so many unique values that the probability of taking just one of these values is really-really small. Hence, the density function actually shows the probability of a random $Y_i$ element being in the small area of $x$ with a radius of $\epsilon$. So, the correct formula would be $P(x \leq Y_i \leq x + \epsilon)$.<br>
But, for **our purposes it's perfectly fine if we think of the $f(x)$ value as the probability of $x$'s occurrence in our $Y$ variable: $P(Y_i=x)$.**

So, if we want to know the probability of a random Tesla price change being exactly $2.76\$$ (the price change in 23th May 2019), then we just need to substitute this $x=2.76$ value into the density function formula of the $N(1.8,27.19)$ distribution: $$P(Y_i=2.76)=f(2.76)=\frac{1}{27.19\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{2.76-1.8}{27.19}\right)^2}$$

Since based on the observed trading data the mean of price changes is $\mu=1.8\$$ and the standard deviation is $\sigma=27.19\$$.

We can apply the `dnorm` function in R to calculate this substitution into the $f(x)$ density function:

```{r}
mu = mean(Tesla$TESLA)
sigma = sd(Tesla$TESLA)

dnorm(2.76, mean = mu, sd = sigma)
```

Perfect! So, the probability of a $2.76\$$ daily gain in Tesla stocks is $1.4\%$.

At this point, let's **think back how the shape of the density function reacted to increasing he standard deviation: it got flat!** Now, this reaction is absolutely understandable since **if the standard deviation is increasing**, then the **probability of extremely high and low $Y_i$ values is also increasing**! And **if $f(x)=P(Y_i=x)$, then the function will "get fat" at the two ends of the $x$ axis**, so the **shape of the function gets to flatten**.

### 1.2. Integral of the Density Function

We can see from the result in the previous section that the occurrence of specific values in a normal distribution is not very high, due to what we discussed before: the occurrence of a specific value in a variable with lots of unique values is very small.<br>
Therefore, we usually ask from our distribution function that **what is the probability of a random $Y_i$ being smaller than $x$?** So, we usually look for the $P(Y_i < x)$ probabilities.<br>
This is nothing more than the **area of the density function below $x$.** So, we can calculate this with the $F(x)=\int_{-\infty}^x{f(x)}dx$ improper integral. This is called as the **cumulative distribution function**: $F(x)$.

If we didn't want to manually substitute $x$ into the formula of the $f(x)$ density function, it's sure as hell, we don't want to take the integral of it!<br>
Luckily, the machine spirit of R has blessed us with the `pnorm` function that will calculate this integral and the $P(Y_i < x)$ probability for us!

Let's see what is the probability of getting a daily *loss* greater than $82\$$ with a Tesla stock. So, we actually look for the $P(Y_i<-82)$ probability.

```{r}
pnorm(-82, mean = mu, sd = sigma)
```

Thankfully, the probability of such great losses is just $0.1\%$, so we can breathe a bit easier. :)

Of course, if we can calculate the probability of being smaller than $x$, then we can also calculate the probability of being greater than $x$, since **being above $x$ is the complementary event of being below $x$,** so we the following formula is valid: $P(Y_i>x)=1-P(Y_i<x)$. We simply calculate the **area of the density function that is above $x$.**

So, let's see what's the probability of gaining more than $20\$$ with a Tesla stock on a random trading day! The whole thing can be handled by the `pnorm` function again:

```{r}
1 - pnorm(20, mean = mu, sd = sigma)
```

Oh wow, these gains above $20\$$ have roughly $25\%$ of occurring. Not bad!

If we want to know the probability of a randomly drawn $Y_i$ being between two numbers $x$ and $y$, then we simply need to **subtract the probability of being below the smaller value from the probability of being below the higher value**. So, if $x>y$, then $P(y<Y_i<x)=P(Y_i<x)-P(Y_i<y)$, but if $x<y$, then $P(x<Y_i<y)=P(Y_i<y)-P(Y_i<x)$ is the way to calculate. So, the `pnorm` function still solves our problems here. On the plot side, we take the **area between $x$ and $y$ under the density function**.

If I want to see the probability of a loss between $47\$$ and $82\$$ with a Tesla stock. So, we just take the probability of being smaller than $-82$ and we subtract it from the probability of being smaller than $-47$:

```{r}
pnorm(-47, mean = mu, sd = sigma) - pnorm(-82, mean = mu, sd = sigma)
```

Perfect, so the probability of a loss between $47\$$ and $82\$$ with a Tesla stock is $3.5\%$.

**To sum up**, we can calculate the following probabilities with the help of a $f(x)$ density function, where $Y_i$ is a random element of our currently examined $Y$ variable, plus $x$ and $y$ are given numbers:

- being exactly $x$: $P(Y_i=x)=f(x)$
- being below $x$: $P(Y_i<x)=\int_{-\infty}^x{f(x)}dx$
- being above $x$: $P(Y_i>x)=1-P(Y_i<x)$
- being between $x$ and $y$: $P(y<Y_i<x)=P(Y_i<x)-P(Y_i<y)$

All of these calculations and their graphical representations can be reviewed in the interactive plot below.<br>
A *small comment*: in the formulas for the probabilities of the *below, above and between* events I didn't apply the $=$ sign since due to the great number of unique values in the variable, the probability of a specific value appearing is essentially $0$, so if we examine the *probability of being in a given range, including and excluding a given point does not really matter*.

<iframe src ="https://kola992.shinyapps.io/normalprobabilities/" height=600px width=800px data-external="1" />

### 1.3. Probability vs Relative Frequency

All right, now we can use the density function of the normal distribution. But **what's the point?** I mean, ok we calculate from the density function that the probability of a loss between $47\$$ and $82\$$ with Tesla is $3.5\%$.

```{r}
pnorm(-47, mean = mu, sd = sigma) - pnorm(-82, mean = mu, sd = sigma)
```

But this is something that we can also know from **the relative frequency of these elements in the data frame, right?** If we calculate the frequency of the elements between $-47\$$ and $-82\$$, and divide the result with the total number of observations, which is $n=250$, then we get the proportion or relative frequency of the losses between $47\$$ and $82\$$:

```{r warning=FALSE}
Tesla_Filter <- Tesla[(Tesla$TESLA > -82) & (Tesla$TESLA < -47),]

nrow(Tesla_Filter) / nrow(Tesla)
```

Oh, well this relative frequency is just $2.0\%$. But the normal distribution suggests that the probability of these losses is $3.5\%$. Now, who is in the right? What's the difference?

Now, the difference comes from the fact that **with the relative frequency of $2.0\%$ we only consider the observed data only, so the result only comes from the observed statistical SAMPLE!!** The $2.0\%$ only means that **only 2% of our observed trading days had losses between 47-82 dollars!!**

On the other hand, the $3.5\%$ is the *theoretical probability* calculated from the density function of the normal distribution, with mean and standard deviation fitted on the data. This a theoretical probability since **the $f(x)$ density function** (being a continuous function) **assigns positive occurrence probabilities to $x$ values that are not yet included in the observed sample!!** This means, that **when calculating probabilities the density function considers values that are OUTSIDE of the sample! In other words, it can "see" values that haven't occurred yet!** That is why we call the density function's $f(x)$ values *PROBABILITIES* and the proportions are just called relative frequencies.

*Nota bene*: To rightfully call the density function's $f(x)$ values probabilities, it is required that the density function more or less fits to the observed histogram. As I've suggested at the start of this section, our Tesla stock price changes have a too pointy frequency distribution for the normal density function to fit completely.

We can create a plot to see how well the theoretical normal distribution with the given $\mu$ mean and $\sigma$ standard deviation fits to the observed histogram.<br>
We can create a simple histogram with `ggplot` that shows the relative frequencies instead of the absolute frequencies with the parameter `aes(y = after_stat(density))` setting in the `geom_histogram` function:

```{r}
library(ggplot2)

ggplot(Tesla, aes(x=TESLA)) +
  geom_histogram(aes(y = after_stat(density)))
```

Then, we can just add another layer on the plot with the `stat_function` function, where we specify in the parameter settings that we want to add a normal density function (`fun = dnorm`) with $\mu=1.8\$$ and $\sigma=27.19\$$ (`args = list(mean = 80, sd = 20)`). Plus we can also set the color of the function as red for example (`col = 'red'`):

```{r warning=FALSE}
mu = mean(Tesla$TESLA)
sigma = sd(Tesla$TESLA)

ggplot(Tesla, aes(x=TESLA)) +
  geom_histogram(aes(y = after_stat(density))) +
  stat_function(
                fun = dnorm, 
                args = list(mean = mu, sd = sigma),
                col = 'red')
```

All right, we can see that the observed histogram is a bit too pointy (has positive kurtosis) compared to the density function of the classic normal distribution.

In the following chapters, we'll see a more exact method than staring at graphs or measures of shape to test whether a theoretical distribution fits to an observed histogram. :)

### 1.4. Central Limit Theorem (CLT)

As we **discussed in the Fundamentals of Statistics course**, it is no surprise that these price changes of financial instruments tend to follow something close to the normal distribution. There is a theory lurking behind the shadows, the **Central Limit Theorem (CLT)**. But we **revise this theorem here**, since it's that important. :)

This theorem states that **if the $Y_i$ elements of a variable are created as a sum of random effects, than the $Y$ variable itself is normally distributed**.<br>
If we think about it, **this is probably the case for the daily price changes**: the **exact price change of a day, the $Y_i$ element, is a sum of the random economic events for that trading day**. Like at the start of the day, Tesla announces that it couldn't open a new factory in China due to not complying with some special regulation there, so the stock prices go down (*not stonks*). But around midday Elon Musk tweets that this fact won't influence meeting their car production plans, so the price of Tesla stocks go up (*stonks*). But at the end of day, Musk tweets that he's pulling some capital out of Tesla to cover some legal costs, so stock prices go down again (*not stonks*), and finally **Tesla stocks have a price change of like $Y_i=-8.28$ dollars as a sum of these random effects that happened during the day**. Since it is reasonable to assume that every trading day looks like something similar (random economic events have some effect to Tesla stock prices and at the end of the they, they sum up), **all the $Y_i$s can be considered as a sum of random effects**. Therefore, **according to the CLT, the $Y$ variable needs to have a normal or symmetric distribution**.

We can see on the histograms at the end of Section 1.3. that **the CLT works to some extent on the price changes of these investment funds, but not perfectly due to the excess kurtosis**. De **több egyéb esetben elég szépen érvényesül**: Pl. egy termelőgép által a nap végén gyártott selejtes termékek száma esetén. Az adott napi selejtszám értéke (ha nincs szabotőr a gyárban) az adott napi véletlen hatások összegződése állítja elő. Így, ha több nap nap végi selejtszámait vizsgáljuk, akkor azok hisztogramja csudiszép normális eloszlást kell, hogy kirajzoljon.

### 1.5. Quantile Function

We can also *ask backwards* from our dear density function. So, they are not only capable of calculating probabilities of given events (like losses greater than 80 dollars with Tesla), but they also can tell us values for given probabilities via their **inverse values**. So I can ask *what is the amount where we have only a $5\%$ probability to experience greater losses?*<br>
So, our task here is to find $x$ such that $P(Y_i<x)=0.05$ and this $x$ is called the **5th quantile of the distribution**, which can be calculated as the **inverse value of the cumulative distribution function**: $F^{-1}(x)$.

Of course, we have a built-in function in R for this purpose, called `qnorm` that can determine $x$ for us, if we give a $P(Y_i<x)$ probability to it in its $1$st parameter. The other two parameters are just the mean and standard deviation of the normal distribution, defining the exact shape of the density function just like before:

```{r}
mu = mean(Tesla$TESLA)
sigma = sd(Tesla$TESLA)

qnorm(0.05, mean = mu, sd = sigma)
```

All right, so $42.9\$$ is the amount, where the probability of a higher loss is just $5\%$. Or, in other words, there is only a $5\%$ probability for a price change lower than $-42.9\$$ on a random trading day. In Finance, we usually call this value as **Value at Risk (VaR)** at $5\%$ level. Usually, financial regulators compel investment banks to put these values into reserve after each of their investment portfolios.

If we think about it, logically **this operation with the distribution quantile values looks the same as the calculation of percentiles from our data**. Since the **5th percentile** of the observed price changes gives the value below which only $5\%$ of the data is found. So, this can be considered as an analogy for the question "*What is the amount where we have only a $5\%$ probability to experience smaller price changes?*"

Let's see the 5th percentile then! The `quantile` function can do the calculation for us from the data observed. The `probs` parameter is where we specify which percentile to calculate: the 5th percentile is the point below of which $0.05$ part of the data is smaller.

```{r}
quantile(Tesla$TESLA, probs = 0.05)
```

Oh my good God in Heaven! That's just $-28.5\$$! So only $5\%$ of ur observed trading days had a greater loss than $28.5\$$! This is a **significantly lower loss level than the $-42.9\$$ from the density function!** And the **value from the density function** is more proper to use here since it **considered values** with some positive probabilities **that the observed datet haven't yet seen occurring**, since they are smaller than the minimum of the observed data for example.<br>
Again, **if we look for the given percentile from a distribution, then I can consider data points outside of our observations!** So, we can **generalize to our whole population**.

As we can see from this example above that we can have a significant difference in *Value at Risk (VaR)* of a portfolio if we calculate it from a distribution or from the observed data of price history! Of course, the normal distribution is not necessarily the best fit, but we can find loads of exotic probability distributions on the menu, so we can choose! :)<br>
Of course, to choose the best fitting distribution, some work must be done, so there is a great temptation to just calculate a percentile from observed past price data...some of the greatest investement banks before 2008 gave in to this tempatation because the regulators allowed them to do so. The result is the financial crysis of 2008. This is partly the topic of the book titled <a href="https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable" target="_blank">The Black Swan: The Impact of the Highly Improbable</a>. I recommend it to everyone interested in the topic, it's quite easy to understand reading. :)<br>
Since 2008, regulations (Basel III in Europe, Basel IV from 2023) compels banks to calculate *VaR* based on the best fitting probability distribution on their data.

Of course, inverse values can answer "positive" questions as well: **What is the amount where we have only a $5\%$ probability to experience greater gains with Tesla on a random trading day?** So, what is the value below which price changes have $99\%$ probability to occur?

Mathematically speaking, we need to find $x$ such that $P(Y_i>x)=0.01$. However, **the `qnorm` function can only handle $P(Y_i<x)$ probabilities**, we need to rephrase the question as **let's find $x$ such that $P(Y_i<x)=0.99$!** And now, the machine spirit will answer our prayers:

```{r}
mu = mean(Tesla$TESLA)
sigma = sd(Tesla$TESLA)

qnorm(0.99, mean = mu, sd = sigma)
```

So we have only $1\%$ probability to experience more gains than $65\$$ on a random trading day with Tesla.

### 1.6. Standard Normal Distribution

We must still take a moment to deal with one last thing regarding normal distributions. The case of the normal distribution with $\mu=0$ mean and $\sigma=1$ standard deviation, called **standard normal distribution**, which has a special space in hell.

Why it has a special place is the fact that every other $N(\mu,\sigma)$ distribution can be transformed to a $N(0,1)$ distribution. We just need to apply the following formula: $$z_i=\frac{Y_i-\mu}{\sigma}$$


So, if we have a **normally distributed variable $Y$ and from its every $Y_i$ element, we subtract the mean and divide the result by the standard deviation, then the resulting $z$ variable with $z_i$ elements has a standard normal distribution**. This operation is called **normalization/standardization**.

In mathematics, we denote by $\sim$ if a variable follows some kind of distribution. So what I can say is that $Y \sim N(\mu,\sigma)$ and $z \sim N(0,1)$.

Let's calculate this standardized $z$ variable for the price changes of the Tesla stock:

```{r}
Tesla$z <- (Tesla$TESLA - mean(Tesla$TESLA))/sd(Tesla$TESLA)

mean(Tesla$z)
sd(Tesla$z)
```

Ok, We can see that this new $z$ variable has a mean of prectically $0$ ($9.5 \times 10^{-18}$) and a standards deviation of $1$.

But we can see that the histogram is still the same, so it's symmetric:

```{r}
hist(Tesla$z)
```

Why we "*love*" the standard normal distribution is that this distribution has

- the middle $68.2\%$ of its data between $-1$ and $+1$
- the middle $95.4\%$ of its data between $-2$ and $+2$
- the middle $99.7\%$ of its data between $-3$ and $+3$

This is further emphasized by the figure below.

<center>
![](stnormal.png){width=50%}
</center>

<br> But we can check this in R as well. For example, for the case of $\pm 2$. The `pnorm` function runs with `mean=0` and `sd=1` parameters by default. So, let's calculate the $P(-2<z_i<+2)$ probability in case of a $z \sim N(0,1)$ distribution:

```{r}
pnorm(2) - pnorm(-2)
```

It's really $95.4\%$! :) **This property of the standard normal distribution will be exploited later, so remember this**!

Because of this property, sometimes the standardized values are used to look for outliers. Like a $Y_i$ value in the variable is an outlier if its corresponding $z_i$ value is outside the $\pm2$ range since in this case, the $Y_i$ value is either in the top or bottom $2.5\%$ of the variable (we can see from the density function of the standard normal distribution that the $5\%$ outside of the $\pm 2$ range is symmetrically distributed on the lower and upper edges of the graph...but this symmetry is no surprise :)). 

We can try this method to locate trading days with outlier price changes in Tesla:

```{r}
Tesla[Tesla$z < -2 | Tesla$z > 2,]
```

We have the days with extreme gains or losses.

But **be very careful with this method!** Looking for outliers based on standardized $z_i$ values **only works if the original (not standardized) variable is already normally distributed!** Since this is the only way the standardized data can follow the symmetric standard normal distribution and the only way for $P(-2<z_i<+2)=0.954$ formula to be correct!

One last thought: standardized $z_i$ values show us by how many standard deviations is the original $Y_i$ value above or below the mean.<br>
Like the first element of the filter above, the $59.82\$$ gain on 30th Jan 2020 is $2.13$ standard deviations above the mean of $1.8\$$. While the the loss of $152.36\$$ on 5th Feb 2020 is $5.67$ standard deviations below the same mean.

## 2. The Exponential Distribution

Now, let’s leave the Tesla stock price changes for a moment and examine another dataset that resides in the <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/CancerSurvival.xlsx\" target="_blank">CancerSurvival.xlsx</a> file. This dataset contains data for 58 severe head and neck cancer patients, recording *how many months* they survived after chemotherapy. The data is real, from 1988, and the source is <a href="https://www.jstor.org/stable/2288857#metadata_info_tab_contents\" target="_blank">this study</a>.

Let’s load the data into a data frame!

```{r}
Surv <- read_excel("CancerSurvival.xlsx")

str(Surv)
```

As we can see, this data frame also only contains two columns. The first column is the patient’s number, and the second column is the survival time in months after chemotherapy (**SurvMonth**).

Let’s take a look at the distribution of survival times with a histogram.

```{r}
hist(Surv$SurvMonth)
```

Well, here we can see that the distribution has a long right tail: the majority of the survival times (45 out of 58 specifically) are within 256 months, but the remaining 13 exceed this, and 3 patients survived for more than 1000 months after chemotherapy.

Due to the long right tail, if we connect the bars of the histogram with a continuous line, we would get a graph similar to the one below.

```{r echo=FALSE}
x <- seq(min(Surv$SurvMonth), max(Surv$SurvMonth))
y <- dexp(x, rate = 1/mean(Surv$SurvMonth))

DFRajz <- data.frame(x=x, y=y)

ggplot(DFRajz, aes(x=x, y=y)) + geom_line(linewidth=1)
```

This shape is the **probability density function of the exponential distribution**. The exact form of this probability density function is determined by the parameter $\lambda$. The larger $\lambda$ is, the steeper the distribution becomes as it tails to the right. This can be tested below.

<center>
```{r echo=FALSE}
knitr::include_app("https://kola992.shinyapps.io/normaldensityplot/?distr=Exp",
                   height = "500px")
```
</center>

Of course, $\lambda$ is related to the mean and standard deviation of the real data, specifically, both values are $\mu=\sigma=\frac{1}{\lambda}$. So, for the exponential distribution, we assume the same mean and standard deviation for the data, and the reciprocal of this common value (i.e., $\lambda$) determines how steeply the distribution’s probability density function skews to the right. Because of this, exponential distributions are often denoted as $Exp(\lambda)$.

Of course, in real data, it is practically never true that $\mu=\sigma$, but we can see from the `describe` function that in the survival data, the values of these two parameters are relatively close: $\mu=226.17 \approx \sigma=273.94$. 

```{r}
library(psych)
describe(Surv$SurvMonth)
```

In R, the functions `dnorm`, `pnorm`, and `qnorm` have analogous functions for the exponential distribution: `dexp`, `pexp`, and `qexp`. Their usage and meaning are exactly the same as the normal distribution functions. The only difference, of course, is that for the exponential distribution, only the uniform $\lambda$ needs to be provided, instead of separate $\mu$ and $\sigma$ values, as was the case with the normal distribution.<br>
In R, $\lambda$ can be provided in the function’s `rate` parameter. Based on the previously seen relationships, we can calculate it using either the mean or the standard deviation as their reciprocals. Which of these two logics we should follow will be discussed in <a href="Chapter06.html" target="_blank">Chapter 6</a>. For now, let’s take $\lambda$ as the reciprocal of the mean.<br>
Thus, for the survival times, $\lambda=\frac{1}{\bar{Y}}=\frac{1}{226.17}=0.00442$. Therefore, the individual survival times $Y_i$ follow an $Exp(0.00442)$ distribution: $Y_i \sim Exp(0.00442)$.

Let’s now calculate a few probabilities related to the survival times:

1. What is the probability that after chemotherapy, a person will survive exactly one year, i.e., $12$ months?

```{r}
dexp(12, rate = 1/mean(Surv$SurvMonth))
```

This is a fairly low probability, about $0.4\%$. We’re not surprised, because the likelihood of exactly one point occurring is low, given the large survival time range.

2. What is the probability that after chemotherapy, a person will survive more than five years, i.e., $60$ months?

```{r}
1 - pexp(60, rate = 1/mean(Surv$SurvMonth))
```

The result is about $76.7\%$, which is quite a good outlook!

What is the probability that, during the third year after chemotherapy, i.e., between $24$ and $36$ months, the person will pass away?

```{r}
pexp(36, rate = 1/mean(Surv$SurvMonth)) - pexp(24, rate = 1/mean(Surv$SurvMonth))
```

The calculation here is also based on $f(x)=P(Y_i=x)$, meaning **the value of the probability density function at $x$ equals the probability of $x$ occurring** in a random draw from the dataset. The probability $P(Y_i<x)$ can also be calculated for the exponential distribution using the improper integral $\int_{-\infty}^x{f(x)}dx$, i.e., it corresponds to the area under the density function up to $x$.

These visual interpretations can be viewed and tried out in the interactive plot below, just like we did with the normal distribution.

<iframe src ="https://kola992.shinyapps.io/normalprobabilities/?distr=Exp" height=550px width=800px data-external="1" />

Of course, we can also calculate *quantile values* for the exponential distribution. For example, let’s see what the time is where there is only a $1%$ probability that a chemotherapy-treated head and neck cancer patient will survive.<br> For the calculation, we need to rephrase the question: what is the time at which there is only a $99%$ probability that a chemotherapy-treated head and neck cancer patient will **not** survive. This is because the `qexp` function, like `qnorm`, works with cumulative probabilities of **being below** the value we are looking for.

```{r}
qexp(0.99, rate = 1/mean(Surv$SurvMonth))
```

The answer is approximately $1042$ months, or $87$ years! But this large value is fundamentally due to the long right tail of the distribution, as long-right-tailed distributions tend to have outlier values that extend upward, so the long-right-tailed density functions must account for these outliers.

Finally, let’s check how well this exponential density function fits the survival time histogram, just as we did for the normal distribution with a histogram-fitted `ggplot` line plot.

```{r warning=FALSE}
ggplot(Surv, aes(x=SurvMonth)) +
  geom_histogram(aes(y = after_stat(density))) +
  stat_function(
                fun = dexp, 
                args = list(rate = 1/mean(Surv$SurvMonth)),
                col = 'red')
```

It looks quite nice here, with only one odd point: the 150-200 month bin. Way more data points fall into this bin than would be expected based on the $Exp(0.00442)$ distribution. But this could easily be due to the large number of bins (we see that `ggplot` used 30 by default). Based on what we learned in Chapter 2, we could fix this if needed. :)"