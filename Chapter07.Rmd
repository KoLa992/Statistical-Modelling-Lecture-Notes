---
title: "Confidence Interval for the Mean in IID Samples"
author: "László Kovács"
date: "07/03/2025"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Revision: Standard Error of the Mean in the Time Results of the Balaton Swimmers

As you may recall, in <a href="Chapter05.html" target="_blank">Chapter 5</a>, we thoroughly examined the time results of the participants in the 2022 Balaton Swim event from a sampling and estimation theory perspective. Let's load the data from the <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/LIDLBalaton2022.xlsx\" target="_blank">LIDLBalaton2022.xlsx</a> file into a data frame again. This Excel file contains the time results of all participants in the `TIME` column. This dataset will now serve as our **population**.

```{r}
library(readxl)
swimming = read_excel("LIDLBalaton2022.xlsx")
str(swimming)
```

So, we have all $N=9751$ participants in the swim. In this chapter, we will **focus exclusively on estimating the mean of the time results**.<br>
So let's **calculate the mean of the time results** ($\mu=\bar{Y}$) **for** the entire **population** of swimmers. Additionally, it will be useful to have the population **standard deviation** ($\sigma$) as a reference. We must be careful because calculating the population standard deviation requires a custom function (we call it `classic_sd` now) to use the classical, uncorrected formula, which divides by $N$ instead of $N-1$.

```{r}
classic_sd <- function(x){
  return(sqrt(mean((x-mean(x))^2)))
}

PopMean <- mean(swimming$TIME)
PopStd <- classic_sd(swimming$TIME)

c(PopMean, PopStd)
```

Based on this, we know that an average Balaton swimmer completed the distance in $\mu=165.5$ minutes, with an an individual swimmer’s own time result deviating by $\sigma=44.1$ minutes on average.

**When estimating the mean**, our task is to **"guess" this $\mu=165.5$ minute mean based on a** random **sample** drawn with replacement (IID) from the population.

So, let’s **take a sample of $n=100$ elements from the population of Balaton swimmers**, using our favorite random seed $1992$, and **check the sample mean**, i.e., the value of $\bar{y}$.

```{r}
set.seed(1992)
selected_into_sample <- sample(rownames(swimming), size = 100, replace = TRUE)
swimming_sample <- swimming[selected_into_sample,]

SampleMean <- mean(swimming_sample$TIME)

SampleMean
```

In this $n=100$ **sample, the average crossing time is $160.8$ minutes**. Based on our reasoning in <a href="Chapter05.html" target="_blank">Chapter 5</a>, we can **bound the population mean ($\mu$) by adding and subtracting $\pm$ the standard error from the sample mean**. Since the standard error tells us how much a randomly selected sample mean is expected to deviate from the true population mean (since the sample mean $\bar{y}$ is inherently an unbiased estimator of the population mean $\mu$).<br>
Following this reasoning, we can say that the **population mean is expected to lie within the interval defined by the sample mean $\pm$ the standard error**.

Good news! The **standard error of the mean is calculated as the population standard deviation divided by the square root of the sample size**, i.e., using the formula $\frac{\sigma}{\sqrt{n}}$. This value can also be approximated from a single sample if we replace the population standard deviation with its unbiased estimate, the **corrected sample standard deviation**. Thus, **from a single $n=100$ sample, the standard error can be approximated using the formula $\frac{s}{\sqrt{n}}$.**

Based on this, we can perform the following calculation on our sample.

```{r}
n <- nrow(swimming_sample)
s <- sd(swimming_sample$TIME) # corrected st. deviation!
SE <- s/sqrt(n)

c(SampleMean - SE, SampleMean + SE)
```

Our results indicate that the true population mean ($\mu$) is expected to be between $160.6$ and $168.3$ minutes, meaning it lies within the $[160.6,168.3]$ interval. Well, **our interval estimation is correct, as the true population mean is $\mu=167.5$ minutes, which is indeed within this interval defined by our sample**.

## 2. Distribution of Sample Means

Alright, alright, with a single sample, we might have just been lucky. **How well does this standard error method actually work with many $n=100$ samples?** Let’s load into a data frame the table that contains the data of $10000$ independent IID samples of size $n=100$! The Excel file I generated in <a href="Chapter04.html" target="_blank">Chapter 4</a>, which contains the $10000$ IID samples, can be accessed <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/SwimmingSamples.xlsx\" target="_blank">here</a>.

```{r}
samples_100 <- as.data.frame(read_excel("SwimmingSamples.xlsx"))
rownames(samples_100) <- paste0("Sample",1:10000) # indicate in the rownames that each row is one sample
head(samples_100)
```

Okay, from the results, we can see that the data frame is structured so that **each row contains one 100-element sample, and the sample elements** (i.e., the time results of the selected swimmers in minutes) **are stored in the columns**.

Now, let’s **calculate the $\bar{y} \pm SE$ interval for each sample**, and check whether the **true population mean** ($\mu$) **falls within the interval**! Let’s **give ourselves a little advantage by calculating the standard error using the known population standard deviation**, $\sigma$. That is, we apply the formula $SE = \frac{\sigma}{\sqrt{n}}$. This is an advantage because, in practice, we do NOT know $\sigma$ from just one 100-element sample!<br>
During the calculation, make sure to use the `apply` function with the parameter `MARGIN = 1` (2nd parameter of the function), since the elements of a single sample are in rows. Also, always restrict the calculation to the first $100$ columns of the data frame, as we will be continuously adding new columns to it!

```{r}
SE <- PopStd / sqrt(n)

samples_100$MeanLower <- apply(samples_100[,1:100], 1, mean) - SE
samples_100$MeanUpper <- apply(samples_100[,1:100], 1, mean) + SE
head(samples_100[,101:102])
```

Okay, now we have the **lower and upper bounds of** the mean interval **estimate**. Now, let’s **calculate the hit rate**!<br>
For this calculation, we use the trick we employed in <a href="Chapter05.html" target="_blank">Chapter 5</a>: a R command like `x > 180` returns a `logical` vector, and by averaging this vector, we get the proportion of "*favorable cases*", meaning the proportion of intervals that correctly include $\mu$.

```{r}
mean((samples_100$MeanLower <= PopMean) & (samples_100$MeanUpper >= PopMean))
```

Well, it seems that the $\bar{y} \pm SE$ method **correctly captures the true population mean $\mu$ in only about $68%$ of the samples**! That’s not great! We’d like a much higher accuracy, say around $90%$.

To understand why this method’s hit rate is so poor, let’s **take a look at the histogram of the $\bar{y}$ sample means!** I won’t optimize the number of bins (like based on Sturge's Rule) for the histogram this time, I’ll just accept the default settings of `hist`.

```{r}
samples_100$SampleMeans = apply(samples_100[,1:100], 1, mean)

hist(samples_100$SampleMeans)
```

Whoa! This is the **most beautiful normal distribution ever!**

On second thought, this is **completely logical, since the Central Limit Theorem (CLT) is at work here**. If you don’t remember the CLT, go back to <a href="Chapter04.html" target="_blank">Section 3 in Chapter 4</a>! :)

According to the **CLT**, if a **variable arises as the sum of random effects**, then the **variable follows a normal distribution**. The **variable of $\bar{y}$ sample means perfectly fits the CLT conditions**! After all, the sample mean is obtained by summing the sample elements and dividing by the sample size. **Since the sampling method is IID, I can be sure that each sample element is a random draw, so a result of a random effect. Then, I sum them up.** Finally, I divide by $n$, but since $n$ is always the same (a constant), it doesn’t change the fundamental logic.

If the **variable of sample means from many samples follows a normal distribution, then we also know its mean and standard deviation!**

- Due to **unbiasedness**, we know that the mean of the sample means is the population mean, i.e. $\mu$.
- The standard deviation of the sample means is simply the **standard error**, i.e. $\frac{\sigma}{\sqrt{n}}$

This is exactly the result we would get if we divided the original CLT equation by $n$.

The original formulation of the CLT states that for independent, identically distributed (IID) random variables $X_1,X_2,\dots,X_n$ where $\forall i$: $E(X_i) = \mu$ and $Var(X_i) = \sigma^2$, and as $n \rightarrow \infty$:$$\sum_{i=1}^n{X_i} \sim N(n \mu, \sqrt{n}\sigma)$$

Dividing this by $n$ gives the following result (applying the rule that a constant factor can be factored out linearly from expected value and squared from variance): $$\frac{1}{n} \times \sum_{i=1}^n{X_i} \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$$

In summary, the **variable of sample means $\bar{y}$ from many samples follows the distribution**: $$\bar{y} \sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$$

We can easily test this fit graphically using the method shown in <a href="Chapter03.html" target="_blank">Section 1.3 of Chapter 3</a>.<br>
Notice that in the `stat_function` of the `ggplot2` package, I specify the mean using the previously computed `PopMean` and the standard deviation using the previously computed `SE` objects! The `linewidth=1` setting slightly increases the thickness of the red density function line.

```{r}
library(ggplot2)

ggplot(samples_100, aes(x=SampleMeans)) +
  geom_histogram(aes(y = after_stat(density))) +
  stat_function(
                fun = dnorm, 
                args = list(mean = PopMean, sd = SE),
                col = 'red', linewidth=1)
```

Very nice! The fit looks great: the **CLT is working once again**! :)

## 3. Confidence Interval for the Mean

Let's now examine the **probability that a randomly selected value from an $N(\mu, SE)$ distribution falls within the $\mu \pm SE$ interval**!

```{r}
pnorm(PopMean+SE, mean=PopMean, sd=SE) - pnorm(PopMean-SE, mean=PopMean, sd=SE)
```

Whoa! This is also approximately $68%$! So, the **fact that a $\bar{y} \pm SH$ estimate is only accurate in $68%$ of the samples is essentially a necessity dictated by probability theory**.

The question is, what can we do about this? **What should we do if we want to provide a better interval estimate for the population mean (or expected value in other words), say with $95%$ reliability instead of $68%$?**

For reasoning, we will rely on <a href="Chapter03.html" target="_blank">Section 1.6 of Chapter 3</a>.

We know that if the **distribution of sample means** is given by: $$\bar{y} \sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$$

Then the **distribution of standardized/normalized sample means will be a standard normal distribution**: $$\frac{\bar{y}-\mu}{\frac{\sigma}{\sqrt{n}}} = z \sim N(0,1)$$

For a **standard normal distribution, it is always true** that $P(-2<z<+2) \approx 95%$. As a reminder, here is the density function of the $N(0,1)$ standard normal distribution from Section 1.6 of Chapter 3.

<center>
![](stnormal.png){width=50%}
</center>

<br>Now, for simplicity, let’s take $\approx$ as $=$: $$P(-2<z<+2) = 95\%$$

We **substitute the formula for $z$** into this relationship: $$P\left(-2< \frac{\bar{y}-\mu}{\frac{\sigma}{\sqrt{n}}} <+2\right) = 95\%$$

For now, we assume that we know $\sigma$, i.e., the population standard deviation, for example, from some prior comprehensive data collection. Our goal is to confine the true population mean ($\mu$) within certain limits based on a single sample mean ($\bar{y}$), since we don’t want to extract all possible samples every time. So, we solve for the population mean $\mu$ from the original inequality: $$P\left(\bar{y}-2 \times \frac{\sigma}{\sqrt{n}}< \mu <+2 \times \frac{\sigma}{\sqrt{n}}\right) = 95\%$$

Thus, this equation means that the **population mean is within the single sample mean’s $\pm 2SE$ interval with approximately $95%$ probability**. This is called the **95% confidence interval for the mean**. The number $2$ is the **confidence multiplier** $k$ corresponding to the $95%$ confidence level. All of this can be computed from a single $n$-element sample by replacing $\sigma$ with $s$! However, it is **crucial that our sample is randomly selected, because only then will we obtain the normal distribution for the sample means that we derived**! As we know, the CLT requires randomly selected sample elements to ensure the "*summation of random effects*" condition.

### 3.1. General Formula for the Confidence Interval of the Mean

If we want to **write this confidence interval in a general form**, we usually say that we construct a $1-\alpha$ confidence interval, where $\alpha$ is the **probability of error**, meaning the probability that the population mean is *not* within the confidence interval: $$P\left(\bar{y}-z_{1-\frac{\alpha}{2}} \times \frac{\sigma}{\sqrt{n}}< \mu <+z_{1-\frac{\alpha}{2}} \times \frac{\sigma}{\sqrt{n}}\right) = 1- \alpha$$

Here, the $z$ value is meant to represent **the $z$ value that must be found such that the probability of "falling below it" in the standard normal distribution is exactly** $1-\frac{\alpha}{2}$.<br>
This concept is illustrated in the following figure.

<center>
![](normal_ci.jpg){width=60%}
</center>

<br>The goal is to find the value of $k$ in the $z \sim N(0,1)$ distribution where $P(-k < z < +k) = 1-\alpha$. This is the same reasoning from which we obtained the value of $2$ for the $95%$ case.
**From the figure above**, we determine this as follows: if we know that the probability of falling between $\pm k$ is $1-\alpha$, then the **probability of falling outside** this range is $\alpha$, which represents our allowed error probability. Due to the **symmetry of the normal distribution density function**, this probability is **evenly split** below $-k$ and above $+k$. Thus, the probability of exceeding $+k$ is $\alpha / 2$. However, since R's `qnorm` function **only works with "cumulative probabilities below a value"**, we need to determine the probability of being below $+k$, which is simply the complement of the exceedance probability: $1-\frac{\alpha}{2}$. This **corresponds to the orange-shaded area in the density function graph**.

We can see that this logic works perfectly for the $95%$ confidence level, i.e., $\alpha=5%$.

```{r}
alpha <- 0.05
qnorm(1-(alpha/2))
```

The result is not exactly $2$ but approximately $1.96$. **In the derivation, I rounded the value**, but I believe the core idea of computing the confidence multiplier has been conveyed. :)

In summary. The **confidence interval for the mean at any given confidence level can be computed in the general form**: $$\bar{y} \pm k \times SE$$

That is, we **take the sample mean ($\bar{y}$) and add/subtract $k$ times the standard error ($SE$), where $k$, as the confidence multiplier, determines the desired confidence level**.<br>
This general formula is **crucial because, in later estimation procedures for the mean, the only things that will change are the specific formulas behind $SE$ and $k$, but this fundamental logic will always remain the same!!**

### 3.2. Check the Hit Rate for the Confidence Interval

As a final step, let’s verify the correctness of our confidence interval formula and **see whether, by using $SE$ calculated with $\sigma$, we can construct a $98%$ confidence interval estimate** for the average completion time of swimmers crossing Lake Balaton using our solution $k = z_{1-\frac{\alpha}{2}}$.

Thus, for our $10000$ samples, in this case, the sample mean requires a distance of $$\triangle = k \times SE = z_{1-\frac{\alpha}{2}} \times \frac{\sigma}{\sqrt{n}}$$

to be measured in the $\pm$ direction. This distance $\triangle$ is called the **length of the confidence interval, or in other words, the total margin of error of the estimate**.

Now, let’s check whether such an estimate really results in an **approximately $98%$ hit rate**.

```{r}
n <- 100
alpha <- 1 - 0.98
k <- qnorm(1-alpha/2)
SE <- PopStd / sqrt(n)
delta <- SE * k

samples_100$MeanLower <- samples_100$SampleMeans - delta
samples_100$MeanUpper <- samples_100$SampleMeans + delta
head(samples_100[,101:103])
```

Okay, now we have our **new confidence interval estimates** for all $10000$ samples. Let’s examine their accuracy!

```{r}
mean((samples_100$MeanLower <= PopMean) & (samples_100$MeanUpper >= PopMean))
```

And indeed, the **hit rate is around $98%$**, so victory! :)

### 3.3. Two Important Properties of Confidence Intervals

One must **be careful with the confidence level** of the confidence interval. If we look at our previous calculations, we can see that

- for $95%$ confidence, $k=1.96$
- for $98%$ confidence, however, $k=2.3$

is the corresponding confidence multiplier.

Due to the relationship $\triangle = k \times SE$, it is easy to see that **increasing the confidence level increases the margin of error, meaning that the confidence interval widens**. This makes complete sense: if I **want a higher probability of capturing the true population mean, I need to "widen the net," increasing the chances of catching it**.<br>
A $100%$ confidence level is only possible if the confidence interval spans $\pm \infty$, which, of course, is not a very useful estimate... :)

It is worth testing this further on the sample of size $100$ created bac in *Section 1*, using a `for` loop with error probabilities $\alpha = {0.2,0.1,0.05,0.01,0.001}$.<br>
For the calculations, I will use the previously computed `SampleMean` and `SE` R objects.

```{r}
alpha_vector = c(0.2, 0.1, 0.05, 0.01, 0.001)

for (current_alpha in alpha_vector) {
  lower <- SampleMean - SE*qnorm(1-current_alpha/2)
  upper <- SampleMean + SE*qnorm(1-current_alpha/2)
  print(paste0("Confidence: ",(1-current_alpha)*100,"% - Conf. Int.: [",
               round(lower,2), ", ",round(upper,2),"]"))
}
```

This phenomenon is clearly observable: as **confidence increases, the confidence interval expands, meaning the margin of error continues to grow**.

- at $90%$ confidence, the estimated average completion time falls somewhere between $153$ and $168$ minutes
- at $99%$ confidence, however, it already extends between $146$ and $175$ minutes!!

The way to mitigate this effect is by **increasing the sample size**! **Let’s rerun the previous `for` loop on a sample of size $n=20$ instead of $n=100$!** We compute the sample mean $\bar{y}$ using the first $20$ columns. Since the sample selection was IID, it behaves as if we had only chosen $20$ elements during sampling, rather than $100$. The standard error formula $SE=\frac{\sigma}{\sqrt{n}}$ can also be easily recalculated for $n=20$.

```{r}
SampleMean_Size20 <- mean(swimming_sample$TIME[1:20])
n <- 20
SE_Size20 <- PopStd / sqrt(n)

alpha_vector = c(0.2, 0.1, 0.05, 0.01, 0.001)

for (current_alpha in alpha_vector) {
  lower <- SampleMean_Size20 - SE_Size20*qnorm(1-current_alpha/2)
  upper <- SampleMean_Size20 + SE_Size20*qnorm(1-current_alpha/2)
  print(paste0("Confidence: ",(1-current_alpha)*100,"% - Conf. Int.: [",
               round(lower,2), ", ",round(upper,2),"]"))
}
```

Clearly, **at $99%$ confidence level, the estimated mean crossing times are**

- between $132$ and $197$ minutes for $n=20$
- and ranging from $146$ to $175$ minutes for $n=100$
  * as we saw earlier, this estimate is more precise (with a smaller $\triangle$ margin of error), 

This result is unsurprising. Since the **standard error formula** $\frac{\sigma}{\sqrt{n}}$ has the **sample size $n$ in the denominator, increasing $n$ reduces the standard error, thereby decreasing the total $\triangle$ margin of error**. As we established in  <a href="Chapter05.html" target="_blank">Chapter 5</a>, the **sample mean is a consistent estimator**: as sample size increases, its standard error ($SE$) decreases and approaches $0$.

Thus, the **chosen $1-\alpha$ confidence level depends on the sample size**:

- With a larger $n$, a $99\%$ confidence level can still provide a precise interval estimate,
- With a smaller sample size, one may need to settle for a more moderate confidence level (e.g., $90\%-95\%$)

## 4. Confidence Intervals in Practice

At this point, let’s **leave behind the Balaton swimmers** and **try calculating the confidence interval for the mean in a case where we do not know the entire population from which the sample was taken**.

### Task 1: Efficiency of Sleeping Pills

Our story is as follows.  

A pharmaceutical company is studying the effect of a new sleeping pill on $10$ randomly selected patients suffering from insomnia. For each of the ten patients, the increase in their sleep duration (in hours) after using the medication was recorded. Based on previous clinical studies, it is known that the change in sleep duration caused by sleeping pills follows a normal distribution with a standard deviation of $2$ hours.  


A sztorink a következő.

Egy gyógyszergyár egy új altató készítmény hatását vizsgálja $10$ véletlenszerűen kiválasztott inszomniában szenvedő páciensen. Mind a tíz páciens esetében feljegyezték, hogy hány órát növekedett az alvásidejük a készítmény használatát követően. Korábbi klinikai vizsgálatok alapján ismeretes, hogy az altató készítmények által kiváltott alvásidő-változás normális eloszlású, $2$ óra szórással.												

$Data = \{1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4\}$

Construct a $95\%$ confidence interval for the expected average change in sleep duration:

a) based on the given conditions above;
b) assuming that the distribution is normal, but the standard deviation is unknown!
c)  How large should the sample be if, with the same confidence level (95%), we want to reduce the margin of error obtained in part b) to half?

### Task 1/a Solution

In this a) task, we are very spoiled. We have a sample of size $n=10$ regarding the effectiveness of the sleeping pill, and we know the data in detail. The sleep duration of the first patient increased by $1.9$ hours after using the sleeping pill, the second by $0.8$ hours, etc. There is one patient whose sleep duration decreased after taking the medication: the 5th subject, by $0.1$ hours.<br>
If we store these data in a `vector`, we can easily calculate the average sleep duration increase for the observed 10 patients, that is, $\bar{y}$.

```{r}
SampleData = c(1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4)
SampleMean = mean(SampleData)
SampleMean
```

Thus, for the observed patients, the average sleep duration increase is approximately $\bar{y}=2.3$ hours. Which is nice, but what could be the **average sleep duration increase for the entire population of insomniac patients?** This is where the **confidence interval** comes into play! So that we can **say something about patients who were NOT observed in the sample!**

In case a), we can accept **all assumptions made in the problem statement**. One part of this states that "*based on previous research,*" we know that the standard deviation of sleep duration changes is $2$. If we take this as true, then we can assume that the standard deviation of sleep duration changes for the entire population is $2$. That is, we "*pretend*" that $\sigma=2$ for our calculations.<br>
**ATTENTION!** If the problem statement wants to communicate that there is a population standard deviation $\sigma$ that we know and can use in our calculations, it will always be wrapped in a phrase like "*based on previous research, it is known that...*".

If **we accept these assumptions, then we have everything needed to apply the confidence interval formula introduced in Section 3**. Since the confidence level is $95\%$, we set $\alpha=5\%$. Let's apply the formulas to our data!

```{r}
n <- 10
alpha <- 1-0.95
assumed_pop_sd <- 2

k <- qnorm(1-alpha/2)
SE <- assumed_pop_sd/sqrt(n)
delta <- k*SE

lower <- SampleMean - delta
upper <- SampleMean + delta

c(lower, upper)
```

Based on the results, this **new sleeping pill is estimated to increase sleep duration by at least $1.09$ hours and at most $3.6$ hours for the entire population of insomniac patients, with a $95\%$ probability!**

The company's management is very happy with this because they can use marketing slogans such as: "*Studies confirm that our sleeping pill increases expected sleep duration by at least $1$ hour with a $95\%$ probability*". A sentence like that is every marketer's dream, so to speak. :)

But **will everything remain this nice if we do not assume** $\sigma=2$, but instead **calculate using the corrected standard deviation ($s$)?**

### Task 1/b Solution

Here, the task tells us not to believe in every dubious "*previous research*," and not to accept the given value of $\sigma$, but instead **calculate a corrected sample standard deviation** for ourselves, that is, $s$ (the corrected sample standard deviation provides an unbiased estimate of the true population $\sigma$), and compute the standard error using $SE=\frac{s}{\sqrt{n}}$.

However, if **we relax our assumption that we know the population standard deviation for computing \$SE\$**, then **the sample means follow a t-distribution with $n-1$ degrees of freedom instead of a normal distribution**.

The **density function of the t-distribution takes the following form**. The degrees of freedom are denoted by $df$.

<center>
![](studentdistr.png){width=60%}
</center>

<br>As seen from the density function above, Student’s t-distribution is actually a **flattened standard normal distribution** (see the figure above). The flattening expresses that the distribution has a larger standard deviation. A larger standard deviation means that extreme values are more likely to occur at the tails of the distribution (because the density function runs higher at these points), making our data more dispersed.

However, as we increase the degrees of freedom ($df$), we gradually "*sharpen*" the distribution back into the standard normal distribution. This makes sense because we are substituting a population standard deviation with an estimated value from a sample when calculating the standard error. This **introduces greater uncertainty and increased dispersion into the distribution**.

In this case, the confidence interval is modified such that $\sigma$ is replaced by $s$ in $SE$, and the confidence multiplier $k$ is calculated from the t-distribution instead of $N(0,1)$: $$P\left(\bar{y}-t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}< \mu <+t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}\right) = 1- \alpha$$


To compute the t-value for $n-1$ degrees of freedom, we use the `qt` function. It works similarly to the `qnorm` function (or any other quantile function in R). The t-distribution is symmetric, so we look for the value where the "*lower-tail*" probability is $1-\frac{\alpha}{2}$. The `df` parameter of the `qt` function controls the degrees of freedom.<br>
For our $n=10$ sample at a $95\%$ confidence level, we compute it as follows.

```{r}
n <- 10
alpha <- 1-0.95

k_z <- qnorm(1-alpha/2)
k_t <- qt(1-alpha/2, df = (n-1))

c(k_z, k_t)
```

We can see that **the t-distribution confidence multiplier ($k$) is significantly larger than the standard normal distribution multiplier ($k$) for the same confidence probability** (or confidence level)! Since the t-distribution assigns a higher probability to extreme values, it results in a higher $k$ multiplier for $SE$ at the same confidence level.

Now, our only task is to compute the total margin of error $\triangle = t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}$, and apply it symmetrically $\pm$ around the sample mean $\bar{y}$. This is done in the same way as in task a).

```{r}
corr_std <- sd(SampleData)

k_t <- qt(1-alpha/2, df = (n-1))
SE <- corr_std/sqrt(n)
delta <- k_t * SE

lower <- SampleMean - delta
upper <- SampleMean + delta

c(lower, upper)
```

Unsurprisingly, due to the **t-distribution confidence multiplier, the $95\%$ confidence interval has widened**. Even though $SE$ remains almost unchanged (since $s=2.0022$), the estimated increase in sleep duration caused by the sleeping pill now falls between $0.9$ and $3.76$ hours with $95\%$ probability. So, in this more realistic scenario, where we compute the standard deviation from the sample rather than "*blindly accepting*" a given value, the increased uncertainty requires a t-distribution, widening the margin of error ($\triangle$). As a result, marketers can no longer claim that "our sleeping pill increases sleep duration by at least $1$ hour with $95\%$ probability."RIP! :(

However, as long as our **sample size remains small ($n \leq 30$)**, there is a **key assumption for using the t-distribution confidence interval: the variable from which the sample is drawn must follow a normal distribution in the population!**<br>
In our case, this means that the *sleep durations* of insomniac patients must *follow a normal distribution*.

Now, with only $n=10$ observations, it's incredibly difficult to assess this meaningfully, but let’s check a histogram of the observed 10 sleep durations! We won’t optimize the number of bins, just use the default settings.

```{r}
hist(SampleData)
```

Well, it's hard to tell from so little data. The sleep durations could be normally distributed. :) Let’s trust the *Central Limit Theorem* (CLT): individual sleep durations likely arise as a sum of random effects, meaning their distribution in the entire population (outside our observed 10 samples) might still be normal.

The *t-distribution* itself was actually the creation of an English statistician named *William Gosset*. Mr. Gosset worked as the head of quality control at the *Guinness* brewery, tasked with ensuring that the quality of bottled beer could be maintained using small sample sizes. Since quality control required opening bottles to measure their ingredients, the tested beer could no longer be sold. *Unfortunate.*<br>
Thus, the brewery wanted to use as few bottles as possible for quality testing. To solve this problem, Gosset developed the t-distribution, since the chemical properties of beer ingredients follow a normal distribution.

However, Gosset wanted to show off his discovery to the world, so he published his findings under the *pseudonym "Student"* —hence the name *Student’s t-distribution*. :) Unfortunately, he wasn’t very strategic, because at the time, only Guinness knew how to perform reliable quality control with small samples. *Mr. Gosset* got caught.

So, as the figure below shows, all Gosset wanted to do was optimize beer production—but in the end, he ruined the lives of everyone studying statistical estimation theory. :)

<center>
![](student.png){width=30%}
</center>

### Task 1/c Solution

Now, let's return to the effectiveness of our sleeping aid. As we have seen, the confidence interval constructed using the t-distribution has brought rather disappointing results: we cannot state with $95\%$ confidence that our product increases sleep time by at least $1$ hour!

What is the reason for this? It is the **high estimation error, the length of the confidence interval built on the mean**, i.e. $\triangle$. Currently, for us, this is $\pm 1.43$ hours.

```{r}
delta
```

Let's say we want to **reduce this estimation error margin by half**, to $1.43/2$, approximately $\triangle'= \pm 0.7$ hours. This can be achieved in two ways. Since, "*broadly speaking*", the estimation error margin is a product of two factors: $$\triangle=k \times SE$$

1. We adjust the confidence level, i.e., effectively $\alpha$, until the confidence multiplier $k$ derived from the t-distribution is low enough that when multiplied by the current $SE$, it results in a $\triangle$ value of $0.7$. This is the **improper solution**! Because achieving this may require increasing $\alpha$ significantly, resulting in a very low confidence level... A statement like "it can be asserted with $55\%$ confidence that..." does not sound very good from a marketing perspective.

2. The **intelligent solution** is increasing the sample size, i.e. $n$. We keep $\alpha$ fixed, thus leaving $k$ unchanged. Of course, due to the degrees of freedom in the t-distribution, sample size does influence $k$, but as can be seen from the probability density function of the distribution, for sufficiently large $n$, the t-distribution essentially becomes equivalent to the standard normal distribution, meaning we cannot further reduce $k$ for a given $\alpha$. Thus, in this case, $k$ is essentially fixed. However, since $SE$ is part of the product and we provide a consistent estimate for the mean, $SE$ will definitely decrease as $n$ increases, ultimately tending toward $0$ as $n$ grows. This is clearly derived from the formula: $SE=\frac{s}{\sqrt{n}}$ So, if we take 
$s$ (or $\sigma$, if it is known) as given, we can calculate the necessary $n$ required to achieve an estimation error of $\triangle'= \pm 0.7$ hours.

Thus, choosing the second solution, increasing the sample size, we need to express $n$ from the following equation: $$\triangle=k \times SE = z_{1-\frac{\alpha}{2}} \times \frac{s}{\sqrt{n}}$$

Here, I wrote $z_{1-\frac{\alpha}{2}}$ instead of the t-distribution multiplier for $k$ because we cannot go below the confidence multiplier value of the standard normal distribution for a given $\alpha$ using the t-distribution. The reason is that even with infinite degrees of freedom, the t-distribution can only be "*peaked back*" into the standard normal distribution at most, as we saw in the figure in part (b) of the solution.

Once we express the sample size from the above formula, we obtain the following equation: $$n=\frac{z_{1-\frac{\alpha}{2}}^2 \times s^2}{\triangle^2}$$

We can substitute all known values into this without issue.

```{r}
delta_new <- 0.7

n_new <- k_z^2 * corr_std^2 / delta_new^2
n_new
```

Thus, to halve the estimation error, we would need a sample size of $n=31.4$... Since we obviously wouldn't cut $0.4$ of a patient for the study, it makes sense to round up to $n=32$. That is, **we would need to examine an additional $32-10=22$ patients to determine the average increase in sleep time with $95\%$ confidence and an estimation error of $\pm0.7$ hours**.<br>
Of course, if the sample mean or the corrected sample standard deviation changes due to the new sample elements in such a way that the lower bound of the confidence interval still does not reach the desired $1$ hour, then our plan has failed. However, in that case, the responsibility lies more with the chemists developing the drug rather than with the statistician conducting the efficacy analysis and highlighting the problems. :)

### Task 2

The <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/ESS2020.xlsx" target="_blank">ESS2020.xlsx</a> file contains a database with the responses of 1849 Hungarian participants to 14 questions (plus an *id* column) from the 2020 European Social Survey (ESS2020).

If an empty value is found in any column, it means that the respondent in that row did not answer the question. The respondents in the database can be considered a random sample drawn from the entire Hungarian population aged 18 and over. For now, we assume that this random sample is drawn with replacement, i.e., it is $IID$. In Chapter 8, we will see that this is not an unrealistic assumption.

The raw database is available <a href="https://ess-search.nsd.no/en/study/172ac431-2a06-41df-9dab-c1fd8f3877e7\" target="_blank">at this link</a> after free registration.

**Our task** is to estimate, based on the sample, the minimum and maximum number of minutes an average Hungarian citizen is expected to spend on the internet daily, with $97\%$ confidence!

### Task 2 Solution

First, let’s load the database into a data frame and check the sample size in the `NetUsePerDay_Minutes` column, which is relevant to our task!

```{r}
ESS <- read_excel("ESS2020.xlsx")
str(ESS)

n <- sum(!is.na(ESS$NetUsePerDay_Minutes)) # this way, we do not consider the non-respondents
n
```

So, by ignoring empty values and using the `is.na` method with the `!` negation operator on the relevant column, we found that for internet usage time, we have a sample of size $n=1099$. This is a **sufficiently large sample**, since, by common agreement in this course, we **consider cases where $n > 30$ as large samples**, as stated in the solution to task 1/b). Because of this, we **do not need to check whether internet usage times follow a normal distribution to perform interval estimation using $s$ and the t-distribution**!

We can proceed smoothly using the formula $\triangle = t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}$

```{r}
sample_mean <- mean(ESS$NetUsePerDay_Minutes, na.rm = TRUE) # with na.rm = TRUE, we do not consider the non-respondents
corr_std <- sd(ESS$NetUsePerDay_Minutes, na.rm = TRUE) # with na.rm = TRUE, we do not consider the non-respondents

SE <- corr_std / sqrt(n)

alpha <- 1-0.97
k_t <- qt(1-alpha/2, df = (n-1))

delta <- k_t * SE

lower <- sample_mean - delta
upper <- sample_mean + delta

c(lower, upper)
```

Thus, based on our sample of $n=1099$, we can state that an average Hungarian spends at least $171.8$ minutes and at most $190.8$ minutes per day on the internet, with $97\%$ confidence.

Our result does not change significantly if we calculate the confidence multiplier $k$ from the standard normal distribution instead of the t-distribution, since $n$ is so large that the probability density function of the t-distribution with $n-1$ degrees of freedom is essentially indistinguishable from that of the standard normal distribution.

```{r}
k_z <- qnorm(1-alpha/2)

delta <- k_z * SE

lower <- sample_mean - delta
upper <- sample_mean + delta

c(lower, upper)
```

We can see that there are only very minimal differences in decimal places. So, **for large sample sizes, the confidence multiplier can safely be calculated from the standard normal distribution instead of the t-distribution**.

Fortunately, in R, **there is a function in a separate package** for calculating the confidence interval of the mean using t-distributed $k$ multipliers.

We can apply the `groupwiseMean` function from the `rcompanion` to calculate the **confidence interval for the mean with t-distribution**.

Let's install and include the package:

```{r eval=FALSE}
install.packages("rcompanion")
library(rcompanion)
```

```{r echo=FALSE}
library(rcompanion)
```

Ok, now we can use the function. In the first parameter, we define the numerical variable on which we want to see a confidence interval for the mean, then add a `~1` code. We'll see the reason for this in just a bit. The second parameter defines the data frame where the variable selected in the first parameter is located. The `na.rm = TRUE` parameter setting means that we ignore the rows with missing values and the `digits` parameter controls the rounding accuracy of the confidence interval results. Higher number here means more digits to the results.

```{r}
groupwiseMean(NetUsePerDay_Minutes ~ 1, data = ESS, conf = 0.97,
              na.rm = TRUE, digits = 4)
```

We confirm our previous results: an average Hungarian spends at least $171.8$ minutes and at most $190.8$ minutes per day on the internet, with $97\%$ confidence. We are awesome! :)

## 5. Summary on the Different Confidence Interval Formulas for the Mean

At this point, it is worth summarizing under what conditions and with what specific formulas the confidence interval for the mean can be calculated.

The basic **formula for the length of the confidence interval, i.e. the margin of estimation error**: $$\triangle = k \times SE$$

1. The population standard deviation, i.e. $\sigma$ is known.
  * $k=z_{1-\frac{\alpha}{2}}$<br>
  * $SE=\frac{\sigma}{\sqrt{n}}$
2. The variable we are sampling has a *normal distribution*
  * $k=t_{1-\frac{\alpha}{2}}^{n-1}$<br>
  * $SE=\frac{s}{\sqrt{n}}$
3. We have a large enough sample size: $n > 30$
  * $k=t_{1-\frac{\alpha}{2}}^{n-1}$ or $k=z_{1-\frac{\alpha}{2}}$<br>
  * $SE=\frac{s}{\sqrt{n}}$

And that’s it, really. If we think through the exercises in *Section 4*, we can distinguish these 3 cases when calculating $\triangle$.

## 6. Case Study

Let’s examine how the average time spent on the internet in the entire Hungarian population varies by political party preference with $99\%$ confidence!

It is interesting to see how much the average differences in internet usage time measured in a sample of $n=1099$ can be generalized to the entire population of party supporters, meaning those people who were NOT observed in the sample—in other words, the **world beyond the sample elements**.

The `groupwiseMean` function can get confidence intervals for the population mean by separate categories of a `factor` or `character` variable. So, for example we can **see the confidence interval of the mean internet usage time by political party preferences**. We just need to switch the `~1` code to `~PoliticalPartyPref` in the first parameter. This indicates that we want confidence intervals separately, not as a whole:

```{r}
groupwiseMean(NetUsePerDay_Minutes ~ PoliticalPartyPref, data = ESS, conf = 0.99,
              na.rm = TRUE, digits = 4)
```

Great! :) Based on this, we can say that an average respondent supporting the united opposition spends $228.8$ minutes online per day, while an average respondent supporting the governing Fidesz party spends only $158.7$ minutes per day.<br>
However, in the entire opposition-supporting population, the average internet usage time is between $196$ and $261$ minutes with $99\%$ probability. In the entire Fidesz camp, the average time spent online is only between $137$ and $180$ minutes, also with $99\%$ probability.<br>
Thus, with $99\%$ probability, the average internet usage time is higher for the Opposition than for Fidesz in the entire population as well, since even the "worst" Opposition average value is already $196$ minutes, while the "best" average time for Fidesz is only $180$ minutes.<br>
All of this, of course, is not surprising considering the orientation of the media operating on different platforms (TV, radio, internet). :)

We can also create a nice visualization from the results using the `ggplot` package. First, we save the result of the `groupwiseMean` function into a separate data frame. Then, we create a bar chart from it in `ggplot`, where the `x` axis represents party preference, while the `y` axis represents the average internet usage time measured in the sample. In the `fill` parameter, we set it so that each bar has a different color depending on party preference (this is just an optical tuning :)). The `stat = "identity"` parameter setting in the `geom_bar` layer ensures that the `y` axis of the graph truly represents the sample means, because by default, the machine spirit would plot the frequencies of party preferences instead.<br>
Finally, in the `geom_errorbar` layer, we add the confidence interval limits to the graph using the `ymin` and `ymax` parameters within the `aes` function. The values for these parameters are `Trad.lower` and `Trad.upper` because the confidence interval limits were stored in columns with these names in the `conf_int_df`, if we remember the result of the previous code.

```{r}
conf_int_df <- groupwiseMean(NetUsePerDay_Minutes ~ PoliticalPartyPref, data = ESS, conf = 0.99,
                             na.rm = TRUE, digits = 4)

ggplot(conf_int_df, aes(x = PoliticalPartyPref, y=Mean, fill = PoliticalPartyPref)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin=Trad.lower, ymax=Trad.upper))
```

As we can see, the average internet usage time for the Opposition remains higher even when confidence intervals are included, compared to voters of other parties in the entire population. In fact, **for the other parties, we cannot confidently say that these average internet usage times differ in the full population** with $99\%$ confidence, **because the confidence intervals overlap**. So, even though in the observed data, the governing Fidesz party has a higher average internet time than the Other parties but lower than Unknown cases, in the unobserved, out-of-sample data —in other words, in the full population— it is entirely possible (with $99\%$ confidence) that Fidesz voters actually spend the least time online on average per day. For this to be the case, the true population mean for Fidesz just needs to be near the lower bound of its confidence interval in the full population, while the mean internet usage times of other parties (Unknown and Other) should end up near the upper bound of their confidence intervals in the full population.

It is also worth noting that the confidence interval for Other parties is the widest. This is, of course, because, as we saw at the beginning of the chapter, only $n=13$ respondents belong to this category. Thus, due to the relationship $SE=\frac{s}{\sqrt{n}}$, it is understandable that we are very uncertain about estimating the average internet usage time for this group. Moreover, this is not even a completely accurate estimate of the confidence interval length! Since, due to the small sample size, the confidence multiplier $k$ was calculated from a $t$-distribution with $13-1$ degrees of freedom, this would require that the internet usage times themselves follow a normal distribution. However, as the histogram below illustrates, daily internet usage times do not follow a normal distribution but have a long right tail.

```{r}
hist(ESS$NetUsePerDay_Minutes)
```