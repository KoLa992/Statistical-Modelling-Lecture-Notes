---
title: "Multivariate Linear Regression"
author: "László Kovács"
date: "27/04/2025"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. COVID-19 Mortality Rates of Hungarian Districts

The <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/COVID_Deaths_Hungary.csv" target="_blank">COVID_Deaths_Hungary.csv</a> file is a data table that stores data on 5 variables (columns) for 102 Hungarian districts (those where hospitals had average or below-average capacity utilization in 2019 according to [NEAK data](http://www.neak.gov.hu/data/cms1026624/Korhazi_agyszamkimutatas_2019.pdf)):

- **District**: Name of the district
- **DeathsCOVID**: COVID-19 mortality rate: the ratio of deaths to infected persons (%) as of 2021-03-04. Source: [atlatszo.hu](https://bit.ly/COVID-adatok)
- **Nurses**: Number of general practice nurses per 10000 inhabitants (2019) Source: HCSO
- **Unemp**: Number of registered unemployed persons per 10000 inhabitants (2019) Source: HCSO
- **WomenOlder65**: Percentage of women aged 65 and older within the residential population (2019) Source: HCSO

Let's read the file! Luckily, it is a well-behaved *csv* file, and can be read simply with the `read.csv` function:

```{r}
COVID <- read.csv("COVID_Deaths_Hungary.csv")
str(COVID)
```

We can see that all the columns listed in the description are present. At most, R slightly complains about the Hungarian, accented district names. But otherwise, we have all four `num`-type columns, and the **District** column can remain of `character` data type, since it serves as an identifier for the districts; each name appears only once in the table, and naturally, this variable will not be subjected to statistical analysis. :)

It is a natural idea to try to analyze how the COVID mortality rate in the examined districts depends on the other three variables (number of nurses, number of unemployed per 10,000, and proportion of women aged over 65) using **simple bivariate regressions**.

Our first logical assumption would be that the more nurses there are relative to the population in a district, the lower the COVID mortality rate should be, as more nurses can provide better care. Let’s test our theory: let's perform a regression where the dependent variable ($Y$) is **DeathsCOVID**, and the independent variable ($X$) is **Nurses**:

```{r}
simple_model <- lm(DeathsCOVID ~ Nurses, data = COVID)
summary(simple_model)
```

At first glance, the result of the `lm` function shows nice things:

- The number of nurses explains about 18% of the variation in COVID mortality across the districts ($R^2=17.6%$), which can be considered a moderate explanatory power since $10% < R^2 < 50%$.
- This explanatory power seems not to collapse to 0 even in districts not included in our sample, as the F-test p-value is $1.12\times10^{-5}$, meaning that the null hypothesis $H_0: R^2=0$ can be rejected at all usual significance levels.
- According to the *residual standard deviation*, the COVID mortality rate estimated by our model differs from the actual data by an average of $\pm1.22$ *percentage points*.

The model seems pretty decent indeed—until we look at the regression equation: $PredictedDeathsCOVID=0.38 \times Nurses+2.02$. **Oh no!** The model's **slope is positive**!! Specifically, if the number of nurses in a district increases by 1 per 10,000 inhabitants, then the COVID mortality rate is expected to **increase** by 0.36 percentage points! Based on this, it would seem that it is "better" to have fewer nurses because then COVID mortality would decrease in the district. Our stomachs just can't digest this result!

The strange result is due to a phenomenon called **confounding**!

Let’s check out the correlation matrix of all the numeric variables in the **COVID** `data frame`:

```{r}
library(corrplot)
CorrMatrix <- cor(COVID[,2:5]) # based on str, columns 2-5 are the numeric variables
corrplot(CorrMatrix, method="number")
```

From the correlations, it is evident that the **DeathsCOVID** variable has a unidirectional and moderately strong relationship with the other three variables. This makes sense for **Unemployment** and **WomenOlder65** variables, since COVID is primarily deadly among those over 65, and where multiple comorbidities are already present. Where unemployment is high, the general health status is typically worse: alcoholism, cardiovascular problems are frequent in high-unemployment districts (see e.g. [this study](https://reader.elsevier.com/reader/sd/pii/S0927537120300609?token=E1AD2E1805FB5308BC40754433D867BE2D6F2FEC1EECA824AFAFC2F37BF35723550C07B43E62BE53CD9F6822494FF445&originRegion=eu-west-1&originCreation=20211005092903)).<br>
BUT, **Nurses** is also moderately and unidirectionally correlated with **Unemp** and **WomenOlder65**! Thus, it is likely that in districts with a high number of nurses per 10,000 inhabitants, **COVID mortality is high simply because these districts also have a higher proportion of unemployed people and elderly women**! In other words, the population’s health status in these districts is already worse!<br>
Now, this phenomenon is called **confounding**: when **a variable** correlates with another only because it **transmits the effects of one or more other variables beyond its own effect**.

Thus, the task is clear: figure out how **Nurses** alone affects **DeathsCOVID** after removing the effects of **Unemp** and **WomenOlder65** (i.e., after filtering out/cleaning the *confounding* effect)!<br>
The tool for this task is **multivariate linear regression**! This is simply an extension of the simple $\hat{Y}=\beta_1 X + \beta_0$ bivariate regression by inserting any number $k$ of explanatory variables into the equation, each with its own $\beta$ coefficient: $$\hat{Y}=\beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + \beta_0$$
 
Based on the above equation, **multiple linear regression** can also be interpreted as a statistical model that seeks to predict COVID mortality rates using **simultaneously the number of nurses, unemployment rate, and proportion of women aged 65 and over**!

## 2. OLS Estimation of Multivariate Linear Regression

Our great luck is that in multiple linear regression, the OLS-based method for determining the $\beta_j$ coefficients works essentially without any changes.<br>
That is, we choose the $\beta_j$ values so that the error of our predictions for the COVID mortality rates ($Y$) is minimized. We continue to measure the estimation error with the so-called SSE (Sum of Squared Errors) indicator: $SSE = \sum_{i=1}^n(y_i-\hat{y_i})^2$.<br>

It remains true that even in the case of the squared error, the **machine is not blindly searching for the $\beta$ coefficients**! The solution to the OLS problem (i.e., the $\beta$ values that yield the smallest $SSE$) can still be expressed with a fixed formula!

The formula $SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i}(y_i-\sum_{j}\beta_jx_j)^2$ can also be expressed in matrix form. If we introduce the $X$ matrix, whose first column consists of all ones (because of the intercept), and the remaining columns contain the $X_j$ variables, then $SSE$ can be written as: $SSE=||y-X\beta||^2$.
If we differentiate this matrix-expressed function with respect to $\beta$, and set the derivatives equal to zero, we can derive the fixed formula for the vector of $\beta$ coefficients calculated using the OLS method: $\beta=(X^TX)^{-1}X^Ty$.

It is not so important for us exactly how this beautiful matrix formula comes out. From a practical point of view, the key takeaway is that even in the case of multiple regression, we do not need the optim function (seen in the 3rd exercise) for a minimization problem, because the solution is given by a fixed matrix formula.
This is why **OLS regression is still widely used today: the $\beta$ coefficients can be given by a fixed formula even in the multivariate case**!

Thus, the coefficients of our model using three explanatory variables can also be estimated using the OLS method via R's `lm` function.

```{r}
multivar_model <- lm(DeathsCOVID ~ Nurses + Unemp + WomenOlder65, data = COVID) # note that predictors are separated by a + sign after a ~ sign
summary(multivar_model)
```

Let’s see what we observe from the `summary` output:

- Since the OLS-based estimation remains, meaning the model error is still measured by $SSE$, we can continue to calculate $R^2$ using the formula $R^2=1-\frac{SSE}{SST}$. Its interpretation slightly changes: it now tells us **what percentage of the variance in the response variable is explained by the set of explanatory/predictor variables**. Be cautious: because of this interpretation, $R^2$ can NO LONGER be calculated as the square of a correlation, since *correlation only describes the relationship between two variables*, not more than two. Thus, in this situation, we can say that the number of nurses, the proportion of women over 65, and the unemployment rate **together** explain $34%$ of the variation in COVID mortality rates across districts. This is a noticeable improvement in explanatory power compared to the two-variable model, where we only examined the effect of **Nurses**.
- The p-value of the Global F-test is still very low, $6.42 \times 10^{-9}$, so unsurprisingly, we can say that at any conventional $\alpha$ level, we can reject the $H_0$ that $R^2$ collapses to 0 outside the sample.
What is interesting to observe here are the degrees of freedom used to calculate the p-value for the F-distribution.
Now, since we have $k=3$ explanatory/predictor variables, it means that in the regression we had to estimate $p=4$ parameters (the $\beta_j$ coefficients including the intercept). Thus, the first degree of freedom of the F-distribution is $p-1=3=k$, and the second is $n-p=102-4=98=n-k-1$.
- Furthermore, the hypotheses of the F-test can also be stated as: $H_0: \beta_j=0 \forall j$, that is, all $\beta$ coefficients can be considered zero in the world outside the sample, meaning none of the explanatory/predictor variables have an effect on $Y$. And $H_1: \exists j:\beta_j \neq 0$, meaning there is at least one $\beta$ that is not zero outside the sample, and the associated $X_j$ has an effect on $Y$.
- Based on the residual standard error, the COVID mortality rate predicted by the three-variable model is expected to deviate by about $\pm 1.1$ percentage points from the actual mortality rate.
What is interesting here is that the denominator for the residual standard error still involves $n-p=98$, since $s_e=\sqrt{\frac{SSE}{n-p}}$.

From the `Coefficients` table, we can again write down the estimated model equation:$$PredictDeathsCOVID=0.04 \times Nurses + 0.003 \times Unemp + 0.27 \times WomenOlder65 - 0.15$$

The equation might seem odd in that the coefficient of **Nurses** is still positive, although it is now only $0.04$, compared to the $0.36$ slope in the two-variable model.<br>
To fully understand what this $0.04$ means and how we can measure the importance of an explanatory variable ($X_j$) in the predictions of our regression model, we need to dig a little deeper into what the $\beta_j$ coefficients actually represent in multiple regression.

## 3. Marginal Effect of Predictors

As I wrote in Section 2, OLS regression is still widely used today because the $\beta_j$ coefficients can be specified with a fixed formula. But this is only one reason for the model’s popularity. The other reason is that the $\beta_j$ coefficients can be interpreted as the **marginal effect** of the corresponding $X_j$ explanatory/predictor variables.

The marginal effect of an $X_j$ explanatory variable is the effect that it exerts **alone and directly** on the outcome variable $Y$. In OLS regression, this marginal effect for the $X_j$ explanatory variable is exactly the $\beta_j$ coefficient. Why this is so will be explained in a future exercise.

Based on this, a general interpretation of a $\beta_j$ is the following: **If the value of the explanatory variable corresponding to the given beta increases by one unit while holding the values of the other explanatory variables constant, then the value of the outcome variable is expected to change by the amount of the beta**.

This interpretation has **three essential elements**:

1. All changes are understood in the **own unit of measurement** of the outcome and explanatory variables being examined.
2. We assume that the values of the explanatory variables not currently under examination do not change. This isolates the direct effect of $X_j$ on $Y$. This is the **ceteris paribus** principle.
3. The $\beta_j$ shows only the **expected change** in $Y$! The change would be exactly $\beta_j$ only if $R^2=100%$.

If this interpretation holds generally, then the **Nurses** variable’s $+0.04$ $\beta$ coefficient in our multiple regression has already been cleared of the confounding effects of unemployment and the population over 65, since the $0.04$ shows the expected change in COVID mortality *without* changes in these other variables, following a +1 unit increase in **Nurses**.

Based on this, let's interpret the $\beta_j$ values in our current model:

```{r}
summary(multivar_model)
```

- $\beta_1=0.04$: If the number of nurses per 10,000 inhabitants in a district increases by 1, while unemployment and the proportion of women over 65 remain unchanged, then the district’s COVID mortality rate is expected to rise by 0.04 percentage points. This still doesn't seem favorable, but we’ll get to the bottom of it shortly.
- $\beta_2=0.003$: If the number of job seekers in a district increases by 10,000 (i.e., the number of job seekers per 10,000 inhabitants rises by 1) while the number of nurses and the proportion of women over 65 remain unchanged, then the COVID mortality rate is expected to rise by 0.003 percentage points. More colloquially: If between two districts with identical numbers of nurses and identical proportions of women over 65, one district has 10,000 more unemployed people, then a COVID-infected individual there has a 0.003 percentage point higher chance of dying.
- $\beta_3=0.27$: If the proportion of women over 65 in a district rises by 1 percentage point while the number of nurses and the unemployment rate remain unchanged, then the district’s COVID mortality rate is expected to rise by 0.27 percentage points. Again more colloquially: If between two districts with identical numbers of nurses and unemployment rates, one has a 1 percentage point higher share of women over 65, then a COVID-infected individual there has a 0.27 percentage point higher chance of dying.

Of course, we also have an **intercept $\beta_0$, which officially shows the expected value of $\hat{Y}$ when all explanatory/predictor variables ($X_j$) are set to 0**.<br>
In our case, this means that the COVID mortality rate in a district with 0 nurses, no women over 65, and full employment would be $-0.15%$ according to the model. Clearly, this interpretation is not meaningful here, since a place where $\forall X_j=0$ does not exist in this domain. If anyone finds such an *all-zero* district, they should run, because zombies are coming :) (negative mortality rate :))

### 3.1. Partial Elasticity

The effect of explanatory variables on the outcome variable can also be expressed in relative terms, i.e., in **percentages**, without sticking to the original measurement units of the variables. This is conveyed by the elasticity coefficient or partial elasticity. The formula is as follows:The effect of explanatory variables on the outcome variable can also be expressed in relative terms, i.e., in percentages, without sticking to the original measurement units of the variables. This is conveyed by the **elasticity coefficient or partial elasticity**. The formula is as follows:

<center>
![](Rugalm.jpg){width=50%}
</center>

As shown in the formula, the elasticity of an $X_l$ explanatory variable can only be calculated for a specific entity, because you have to substitute specific $X_j$ values into the regression estimate $\hat{Y}=\beta_0+\beta_1X_1+...+\beta_kX_k$.

Thus, in general, we cannot say what the elasticity of unemployment is on COVID mortality, but we can say what the **elasticity of unemployment on COVID mortality is in the Pécs district, where the number of nurses per 10,000 people is 6.5, unemployment is 222/10,000, and the proportion of women over 65 is 13.3%**:

```{r}
Betas <- coef(multivar_model) # save the coefficients to a separate vector for further calculations

Betas # note that the first element in the vector is Beta_0 !!!

elasticity <- Betas[3]*222 / (Betas[1]+Betas[2]*6.5+Betas[3]*222+Betas[4]*13.3)

elasticity
```

The result, $0.15$, is **already given in percentage terms**!! That is, the number means that if **the 222-unemployed rate in the Pécs district increases by 1%, while the number of nurses and the share of women over 65 remain unchanged on the levels of 6.5 and 13.3% respectively, then the COVID mortality rate in the district is expected to increase by 0.15%!!** Thus, elasticity can be interpreted as a *percentage beta*, but one must pay very close attention that **the +1% change in $X_j$ and the constancy of all other $X$ variables are relative to the given baseline** (Nurses=6.5; Unemp=222; WomenOlder65=13.3)!!

Why does this matter? Let’s compute the unemployment elasticity in the Siklós district, where *Nurses=6.7, Unemp=692, WomenOlder65=12.7*:

```{r}
Betas[3]*692 / (Betas[1]+Betas[2]*6.7+Betas[3]*692+Betas[4]*12.7)
```

**It’s already higher than 0.15%!!** Thus, if my starting point is a district with 6.7 nurses per 10,000, a 12.7% share of women over 65, and 692 job seekers per 10,000 people, then with a +1% rise in unemployment (keeping other factors constant), the COVID mortality rate is expected to rise by 0.36%!!

Which makes total sense, because if an *already poor health status* (i.e., high unemployment) population experiences another +1% unemployment, it worsens COVID survival chances more than if the population had been healthier (i.e., lower unemployment).

Similarly, although in an opposite direction, one can detect the **law of diminishing marginal utility** via elasticities! This is akin to the experience that getting +1 bottle of cold beer on a hot summer day when you are extremely thirsty is totally different than getting the 10th bottle at a party... :)

### 3.2. Partial Correlation

Now the question arises: How do we decide which explanatory variable is more important in predicting $Y$? How do we determine which $X_j$ has a stronger, **direct influence** on $Y$? This is particularly pressing for the **Nurses** variable: its coefficient implies a positive marginal effect, which seems illogical, just like in the two-variable regression. But the question remains: How important or decisive is this effect for predicting $Y$, i.e., the COVID mortality rate?

The intuitive answer would be that the $X_j$ whose $\beta_j$ has the largest absolute value is the most important explanatory variable. **The problem is that betas depend on measurement units!** So even if I see that the beta for the number of nurses per 10,000 people (0.04) is larger than the beta for the number of job seekers per 10,000 people (0.003), this doesn’t tell me much! Since the magnitude of the two variables differs greatly. See the differences in their averages.

```{r}
mean(COVID$Nurses)
mean(COVID$Unemp)
```

Comparing $\beta_j$ coefficients across such different averages would be unfair, since *+1 unit* is relatively more significant among nurses than among unemployed people.

And elasticity has the problem that its value depends on the baseline values of the explanatory variables.

We could turn to correlation to answer the question (because correlation always lies between $\pm 1$): the explanatory variable that is more highly correlated (in absolute value) with the outcome variable would be more important. Reminder: let’s look at the correlations between our explanatory variables and the outcome variable:

```{r}
cor(COVID$DeathsCOVID, COVID$Nurses)
cor(COVID$DeathsCOVID, COVID$Unemp)
```

Based on this, unemployment appears more important, but not by much: the correlations with COVID mortality rates are moderate (between 0.3 and 0.7 in absolute value) for both explanatory variables.

However, the problem is that **correlation measures both direct and indirect effects of explanatory variables on COVID mortality**! It does not isolate the direct effect like the multivariate regression $\beta_j$'s do! So, **confounding is still at play here**: the number of nurses could be positively correlated with COVID mortality just because higher unemployment is associated with a higher number of nurses, and higher unemployment increased COVID mortality, even though the number of nurses itself may have no independent effect. Or the reverse could be true: unemployment may have no independent effect, but it exerts an indirect effect through the number of nurses.<br>
The key is that **based on simple correlation alone, we cannot decide this**.

Thus, we usually prefer partial correlations between explanatory variables and the outcome variable instead of simple correlations. Partial correlation is like simple correlation: it shows the strength and direction of the relationship between two variables on a [-1; +1] scale but **cleans it from the effects of other variables**.

In R, we can compute it using the `ppcor` package:

```{r eval=FALSE}
install.packages("ppcor")
library(ppcor) # Ignore the Warnings as usual! :)
```
```{r echo=FALSE}
library(ppcor)
```

```{r}
pcor(COVID[,2:5]) # columns 2-5 contain variables in our regression
```

We are interested in the first column of the `estimate` list element. Thus, unemployment has a +0.383 partial correlation with COVID mortality after controlling for the effects of the number of nurses and the proportion of women over 65. In contrast, the *number of nurses has only a very weak partial correlation with COVID mortality, with an absolute value below 0.05 (0.047) after controlling for unemployment and the proportion of women over 65*.

Thus, **when examining the direct effects of our explanatory variables on COVID mortality on a unified [-1, +1] scale, it is clear that unemployment** (and, by the way, also the proportion of women over 65 with its 0.29 partial correlation) **has a "moderate" direct effect on increasing COVID mortality, while the number of nurses has only a weak effect**!

The difference between correlation and partial correlation is well illustrated by the following figure.

<center>
![](ParcKorr.png){width=50%}
</center>

The key is that simple correlation measures both *direct and indirect* effects of **Nurses** (blue and red arrows) on COVID mortality, while partial correlation only considers the *direct* effects (red arrows).

### 3.3. Path Analysis

At the end of the previous subsection, the direct and indirect effects represented by the arrows in the figure can also be expressed specifically and numerically using the regression $\beta$s.

Let us take a slightly recolored version of the figure from Section 3.2.

<center>
![](Utelemzes.png){width=50%}
</center>

Here, the *red* arrow indicates the **Nurses'** direct effect on **DeathsCOVID**, while the *blue* arrow indicates the **Unemp**'s direct effect on **DeathsCOVID**. The *green* arrow shows the effect of **Nurses** on **Unemp**.

The magnitudes of the direct effects on the red and blue arrows are given by the coefficients $\beta_1$ and $\beta_2$ of the regression $PredictDeathsCOVID = \beta_1 \times Nurses + \beta_2 \times Unemp + \beta_0$. So let's extract these $\beta$s into separate R objects:

```{r}
# first, get the multivariate regression with Nurses and Unemp
twopredictor_model <- lm(DeathsCOVID ~ Nurses + Unemp, data = COVID)

# get the vector of coefficients
twopredictor_model$coefficients

# save the 2nd and 3rd elements in the vectors: note that the 1st element is Beta_0!
Beta_Nurses_COVID <- twopredictor_model$coefficients[2]
Beta_Unemp_COVID <- twopredictor_model$coefficients[3]
```

The magnitude of the effect on the green arrow is simply given by the coefficient $\beta_1$ of the bivariate regression $PredictUnemp = \beta_1 \times Nurses + \beta_0$. Let's save this into a separate R object as well:

```{r}
# first, get the simple (bivariate regression)
Unemp_Nurses_model <- lm(Unemp ~ Nurses, data = COVID)

# get the vector of coefficients
Unemp_Nurses_model$coefficients

# save the 2nd element in the vectors: note that the 1st element is Beta_0!
Beta_Nurses_Unemp <- Unemp_Nurses_model$coefficients[2]
```

Thus, the *direct and indirect* effects of the $Nurses \rightarrow DeathsCOVID$ relationship can be determined:

```{r}
# direct effect of nurses on COVID deaths (red arrow)
Direct_Nurses_COVID <- Beta_Nurses_COVID
Direct_Nurses_COVID

# indirect effect of nurses on COVID deaths (green*blue arrow)
Indirect_Nurses_COVID <- Beta_Nurses_Unemp * Beta_Unemp_COVID
Indirect_Nurses_COVID
```

Because the **indirect** effect is simply taking the $Unemp \rightarrow DeathsCOVID$ direct effect (blue arrow) multiplied by how much **Unemp** changes per one unit increase in **Nurses** (green arrow). This is exactly what `Beta_Nurses_Unemp` gives!

Thus, the **total** effect of the $Nurses \rightarrow DeathsCOVID$ relationship can be determined:

```{r}
# total effect = direct + indirect effects
Total_Nurses_COVID <- Direct_Nurses_COVID + Indirect_Nurses_COVID
Total_Nurses_COVID
```

And look, this value is exactly the same as the slope of the original simple bivariate regression $PredictCOVIDDeaths = \beta_1 \times Nurses + \beta_0$! :)

```{r}
simple_model$coefficients
```

Thus, by using the multivariable regression, we have successfully **cleared the confounding** effect on the **Nurses** variable from the original bivariate regression!

Obviously, in a similar way, we could measure how much confounding effect is caused by the **WomenOlder65** variable on the $Nurses \rightarrow DeathsCOVID$ relationship.

### 3.4. Partial t-test

We can also measure the importance of explanatory variables through hypothesis testing. For any given explanatory variable $X_j$, its importance can be determined by testing the following null and alternative hypotheses:

- $H_0: \beta_j=0$ ~ The effect of $X_j$ on $Y$ outside the sample is **not significant**
- $H_1: \beta_j\neq0$ ~ The effect of $X_j$ on $Y$ outside the sample is **significant**

Thus, in this hypothesis test, which we will call a **partial t-test**, the **$H_0$ states that the effect of $X_j$ on the outcome variable is just a sampling error, meaning if we observed new individuals (new districts), the measured effect would disappear, and $\beta_j$ would become zero**.

For this hypothesis test, based on what we saw in the Global F-test, we need a test statistic and a p-value.

R provides a **standard error for calculating the test statistic**. This appears in the 2nd column of the table generated by R's `summary` function:

```{r}
summary(multivar_model)
```

For example, the value $0.096$ means that the $\beta$ of **Nurses** is 0.04 in the sample, but if we ran the regression on new districts outside the observed sample, this value could fluctuate around 0.04 with an *expected* variability of $\pm 0.096$. The ratio of $\beta_j$ to its standard error ($SE_j$) gives the so-called *t-value*, which is the test statistic of the partial t-test. For the **Nurses** variable, this is 0.463.

The distribution of the test statistic under $H_0$ follows a t-distribution, whose exact shape is determined by the degrees of freedom $df=n-p=n-k-1$, which was the second degree of freedom in the Global F-test.

Under $H_0$, the ideal case is when $\beta_j=0$ in the sample as well, because $t=\frac{\beta_j}{SE_j}$. Due to the shape of the t-distribution, the p-value must be calculated such that deviations both above and below 0 reduce the probability of rejecting a true $H_0$:

<center>
![](pt.jpg){width=50%}
</center>

Thus, in R, the p-value is: `2*pt(-abs(0.0445217/0.0961260), df = 102-4)`$=0.6442767$. This is shown in the `Pr(>|t|)` column of the `summary` output table.

By looking at the `Pr(>|t|)` column, we can conclude that unemployment and the proportion of women older than 65 have a significant impact on COVID mortality even *outside the observed districts*, since the p-value of the partial t-test is smaller than the $\alpha=1%$ significance level. That is, by rejecting $H_0$, I would be making an error with a very low probability for both explanatory variables. In the case of the **WomenOlder65** variable, this means that the moderate-weak direct effect on prices indicated by the partial correlation (+0.29) would remain even when examining new districts. Meanwhile, it is also clear that the p-value associated with the **Nurses** variable is much larger than that of **Unemployment** and **WomenOlder65**. Specifically, the p-value is $64.4%$, which is so high that it exceeds even the highest commonly used $\alpha$ level of $10%$, so I can confidently accept the null hypothesis $H_0$! Thus, the partial t-test also says that the effect of **Nurses** on COVID mortality **is not significant** outside the observed sample.

In conclusion, based on our model, we can state that the **positive effect of the number of nurses on COVID mortality observed in the bivariate regression was merely apparent, and it actually mediated the increasing effects of unemployment and the proportion of women over 65 on COVID mortality, without any independent significant marginal effect**!

These per-variable t-test p-values are great because they allow us to easily rank the explanatory variables in terms of importance.

## 4. Predictor Importance Order based on t-tests

Let's revisit he <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/BP_Flats.xlsx\" target="_blank">BP_Flats.xlsx</a> file, which is a data table that stores data for 10 variables (columns) for 1406 apartments in Budapest:

- Price_MillionHUF: price of the flat in million HUF
- Area_m2: area of the flat in square meters
- Terrace: number of terraces in the flat
- Rooms: number of rooms in the flat
- HalfRooms: number of half-rooms in the flat
- Bathrooms: number of bathrooms in the flat
- Floor: the number of floor the flat is on
- IsSouth: is the flat looking at the South? (1 = yes; 0 = mo)
- IsBuda: is the flat in Buda? (1 = yes; 0 = no)
- District: district of Budapest the flat is in (1 - 22)

Read the data table from Excel into an R `data frame` the usual way!

```{r}
library(readxl)
BP_Flats <- read_excel("BP_Flats.xlsx")
str(BP_Flats)
```

We won't bother with data types now; we'll leave everything as numeric.

Now let’s look at a model to estimate apartment prices, where all variables from the **BP_Flats** `data frame` are included as explanatory variables except for **District**:

```{r}
bigmodel <- lm(Price_MillionHUF ~ ., data = BP_Flats) # in this regression every variable is included as predictor

bigmodel <- lm(Price_MillionHUF ~ .-District, data = BP_Flats) # in this regression every variable is included as predictor except for District

summary(bigmodel)
```

We see that the explanatory power of the model in the sample is 81.76%, meaning that the explanatory variables in the model collectively explain about 82% of the variation in apartment prices among the observed 1405 apartments. After filtering out the effects of the other variables, the direct effect of the number of rooms is still significant at $\alpha = 5%$, but not at $\alpha = 1%$! Meanwhile, the number of half-rooms and the number of floors are not significant explanatory variables at any of the usual $\alpha$ levels in the world outside the observed sample.

The importance ranking of the explanatory variables can be given by ordering the variables in ascending order of their p-values, because the smaller the p-value, the smaller the chance that it is an error to consider that $X_j$ a significant explanatory variable:

```{r}
# save the coefficients table from the summary results to a separate object
BetaTable <- summary(bigmodel)$coefficients

# order the table according to the 4th column (p-values)
BetaTable[order(BetaTable[,4]),]
```

We can see that Area is the most important explanatory variable, while, for example, the number of rooms is the third least important explanatory variable.

## 5. Relationship of Confidence Intervals and t-tests

In R, you can also create confidence intervals for the coefficients (betas) of the multiple regression model at a given confidence level using the confint function, just as we saw in the bivariate case.<br>
For example, **a 97% confidence interval for $\beta_j$ indicates the range within which the coefficient $\beta_j$ is likely to fall with 97% probability in the world beyond the observed apartments**.

Let's look at the R computation:

```{r}
confint(bigmodel) # 95% is the default

confint(bigmodel, level = 0.99) # but it can be anything like 99%
```

For example, based on the first results table, the $\beta$ of the **Area** variable would fall between 0.277 and 0.317 with 95% probability if we examined not just 1405 apartments in Budapest, but the entire population of apartments.

The result also shows a relationship: **if an $X_j$ is not significant at a given $\alpha$ significance level, then the boundaries of the $1-\alpha$ confidence interval for $X_j$ will change sign**.<br>
The best example is the case of the **Room** variable: we said its effect on prices is still just significant at the 5% level -> thus its 95% confidence interval just barely does not cross zero. However, at the 1% level, it is no longer significant, so its confidence interval does change sign.

This leads us to an interesting point of view: *if an $X_j$ variable is not significant at $\alpha$, it means that at $1-\alpha$ confidence level we cannot even decide whether an increase of 1 unit in $X_j$, all else being equal, increases or decreases the dependent/outcome variable in the world beyond the sample*.

In the background, what happens is that R calculates the inverse value of the t-distribution with $n-p=n-k-1$ degrees of freedom at $1-\alpha/2$ probability, just as it does in the partial t-test, and it adds and subtracts this value multiplied by the standard error for every $\beta_j$:

```{r}
BetaTable

alpha <- 0.02
p <- nrow(BetaTable)
conf_multiplier <- qt(1-alpha/2, df = nrow(BP_Flats)-p)

BetaTable[2,1] - conf_multiplier * BetaTable[2,2] # Area 98% lower bound
BetaTable[2,1] + conf_multiplier * BetaTable[2,2] # Area 98% upper bound
```

Formally, the operation of calculating confidence intervals for $\beta_j$s can be described by the following formula:

<center>
![](BetaCI.jpg){width=50%}
</center>

## 6. Standard Regression Model Assumptions

Roughly, we've now seen how OLS regression can work if everything is fine. But for the learned *partial t-test* and the *confidence intervals* based on $\beta_j$ to realistically reflect the world beyond the observed data — that is, the **population** — *three* conditions must be met:

1. The OLS estimate of $\beta_j$ must be **unbiased**: the average (*expected value*) of $\beta_j$ over many samples must equal the true population value of $\beta_j$.
2. The OLS estimate of $\beta_j$ must be **consistent**: the standard error of $\beta_j$ must approach 0 as the sample size increases; that is, if $n \rightarrow \infty$, then $SE_{\beta_j} \rightarrow 0$.
3. The OLS estimate of $\beta_j$ must be **efficient**: there should be no alternative estimation method for $\beta_j$ with a smaller standard error than the OLS estimate.

These three properties together make the OLS $\beta_j$ estimate **BLUE**: Best Linear Unbiased Estimator.

However, in practice it is very difficult to truly verify these conditions on the model's $\beta_j$ coefficients, because among other things, this would require us to perform many repeated samples from our dataset or population. But we are blessed with only one single sample — we must work with what we have.<br> Thus, **6 standard assumptions** were formulated for multiple OLS regression models. If these are jointly satisfied in the model, then the coefficient estimates will satisfy the three properties discussed above, that is, they will have the **BLUE** property. These are called the **standard regerssion model assumptions**, or **Gauss-Markov conditions**!

Let's see what these are!

1. **Large sample size** ($n>100$).
2. **Strong exogeneity**: The error term $\epsilon_i=y_i-\hat{y_i}$ has a mean of 0 and is uncorrelated with the explanatory variables.
3. The correct regression **functional form is linear**: you do not need to include non-linear transformations of the explanatory variables (e.g., squares or logarithms) in the equation.
4. **No exact multicollinearity**: No explanatory variable can be approximated with an $R^2$ greater than about 95% by regressing it on the other explanatory variables.
5. **Homoscedasticity**: The variance of the error term $\epsilon_i$ is constant across observations; the size of the error is independent of which observation we look at.
6. **No autocorrelation**: The error term $\epsilon_i$ is not correlated with a lagged version of itself, e.g., with $\epsilon_{i-2}$.

Points 1), 2), and 3) are required for unbiasedness.<br>
Points 4), 5), and 6) are required for consistency and efficiency.

In the coming weeks, we will examine how to

- check whether these model assumptions are satisfied,
- understand exactly what problems arise if any assumption is violated,
- learn how to fix these problems if possible,
- and if we cannot fix the violations, what alternatives we can use instead of the partial t-test and confidence intervals.
