---
title: "Basic Consepts of Hypothesis Testing"
author: "László Kovács"
date: "23/03/2025"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Fundamentals of Hypothesis Testing 

In essence, **hypothesis testing** is simply an **alternative method, alongside confidence intervals, for accounting for the sampling error of a statistical parameter (statistical measure)**, when we aim to infer its true population value from its estimator calculated from a sample. However, the **concept of hypothesis testing approaches the problem somewhat differently compared to the confidence intervals we have studied so far**.

In hypothesis testing, we formulate a **statement (or, more formally, a hypothesis) about the true population value of a statistical parameter** (mean, proportion, median, variance, etc.), **and then we attempt to determine whether the observed sample data support or contradict this hypothesis**. For now, we focus specifically on parametric hypothesis tests, or in short, **parametric tests**, because the **hypothesis we formulate will always concern the true population value of a statistical parameter**.

Let's once again **examine an IID sample of size $n=100$ from the population of participants who completed the 2022 Balaton Swim**, based on the following file: <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/LIDLBalaton2022.xlsx" target="_blank">LIDLBalaton2022.xlsx</a>.<br>
As we also examined in <a href="Chapter05.html" target="_blank">Chapter 5</a>, this file contains the *name, gender, and time* (measured in minutes) of the Balaton Swim participants. This dataset will now represent our **population**.

```{r}
library(readxl)

# Read the population into a data frame
swimming_population <- read_excel("LIDLBalaton2022.xlsx")
str(swimming_population)
```

Great, let's extract that IID sample of $n=100$ elements! Let’s set the value in `set.seed` to $1992$ so that we all get the same random sample of $100$ elements!

```{r}
set.seed(1992)
selected_into_sample <- sample(rownames(swimming_population), size = 100, replace = TRUE)
swimming_sample <- swimming_population[selected_into_sample,]
str(swimming_sample)
```

Now then. Regarding the **average swim time of the entire population, I can formulate $6$ different statements** in relation to a specific value, e.g., $2.5$ hours, i.e., $150$ minutes. As before, we denote the true population mean swim time by $\mu$:

1. I’m optimistic and say that people were fast, and the average swim time in the population is less than $150$ minutes: $\mu < 150$
2. As a pessimistic Hungarian, I could say that the swimmers were slow, and the average time in the population is greater than $150$ minutes: $\mu > 150$
3. I can also imagine that the true average swim time is exactly $150$ minutes: $\mu = 150$
4. Or I might believe that the population’s average swim time is anything but $150$ minutes: $\mu \neq 150$
5. A semi-optimist might say that the population’s average swim time is at most $150$ minutes: $\mu \leq 150$
6. By elimination, the semi-pessimist would think that the population’s average swim time is at least $150$ minutes: $\mu \geq 150$

Alright, so I have 6 theories... or rather, *hypotheses*. Now, let’s check the **average in the observed $100$-element sample**.

```{r}
mean(swimming_sample$TIME)
```

Based on the observed $100$ swimmers, the sample mean is approximately $161$ minutes. At first glance, statement 2, the pessimistic one, seems true, as the **average is greater than $150$ minutes**. BUT! **What we see here is only the average of $100$ people; the population’s true average swim time could be different!**<br>
To put it more precisely, the **difference between $161$ and $150$ could simply be due to sampling error**!!<br>
**That’s why we perform hypothesis testing**: To determine whether the hypothesis $\mu > 150$ can be considered as valid, even considering sampling error.

In reality, if we think about it, all 6 statements boil down to the same fundamental question: Does the true population mean $\mu$ differ enough from $150$ that the **difference exceeds sampling error**? In statistical terms, we **test whether the difference between the true $\mu$ and $150$ is SIGNIFICANT**.

Here, we actually have full population data and can compute that $\mu = 167$ minutes, which is **indeed higher than** $150$.

```{r}
mean(swimming_population$TIME)
```

However, **in practice, we DO NOT know this, because we only have the $n=100$ element sample!** So, our **only option is to use hypothesis testing to check whether the observed sample mean $\bar{y} = 161$ significantly differs from $150$**.

Following this logic, we define a **null hypothesis** $H_0$ and an **alternative hypothesis** $H_1$ **from our original statement**.

The key principle is the following. The null hypothesis $H_0$ always **allows equality for the examined statistical parameter**. The alternative hypothesis $H_1$ **depends on the original statement**:

- If the **original statement allows equality**, then it becomes $H_0$, and we **negate it in $H_1$**. In this case, we **expect $H_0$ to be true**, meaning that **the true population mean swim time does not significantly differ from $150$ minutes**.
- If the **original statement does not allow equality**, then it moves to $H_1$, and we **negate it in $H_0$**. Here, we **expect $H_1$ to be true**, meaning that **the true population mean swim time significantly differs from $150$ minutes**.

Using these principles, we can construct the following $H_0$ and $H_1$ pairs for our $6$ original statements:

1. Statement: $\mu < 150$ || $H_0:\mu \geq 150$ || $H_1:\mu < 150$ || Statement is in $H_1$
2. Statement: $\mu > 150$ || $H_0:\mu \leq 150$ || $H_1:\mu > 150$ || Statement is in $H_1$
3. Statement: $\mu = 150$ || $H_0:\mu = 150$ || $H_1:\mu \neq 150$ || Statement is in $H_0$
4. Statement: $\mu \neq 150$ || $H_0:\mu = 150$ || $H_1:\mu \neq 150$ || Statement is in $H_1$
5. Statement: $\mu \leq 150$ || $H_0:\mu \leq 150$ || $H_1:\mu > 150$ || Statement is in $H_0$
6. Statement: $\mu \geq 150$ || $H_0:\mu \geq 150$ || $H_1:\mu < 150$ || Statement is in $H_0$

In statements 1, 2, 5, and 6, where **$H_0$ does not have a strict equality sign, we often define a technical null hypothesis $H_0^T$**. This simply means that we **rewrite $H_0$ using a strict equality sign**.<br>
For example, **for statements 1 and 5, the technical null hypothesis is**:

1. Statement: $\mu < 150$ || $H_0^T:\mu = 150$ || $H_1:\mu < 150$ || Statement is in $H_1$
5. Statement: $\mu \leq 150$ || $H_0^T:\mu = 150$ || $H_1:\mu > 150$ || Statement is in $H_0$

Statistical **tests** (also called hypothesis tests) **are categorized based on the relational sign in $H_1$**:

- If $H_1$ contains $\neq$, then it is a **two-tailed test**
- If $H_1$ contains $<$, then it is a **left-tailed test**
- If $H_1$ contains $>$, then it is a **right-tailed test**

### 1.1. The concept of p-value

If we have properly written down the corresponding $H_0$ and $H_1$ pairs, we then determine which one is considered true based on our observed sample. For this purpose, we use a **statistical measure called the p-value**.  

The **p-value tells us the probability that, assuming $H_0$ is TRUE, we obtain a deviation from $H_0$ that is GREATER than what we observed in our sample data**. Clearly, the value of the statistical measure calculated from the sample (estimator) will never be exactly equal to what we stated in $H_0$. The question is, **what is the probability that a random new sample will produce a larger deviation from $H_0$ than what we observed in the current sample?** This probability is expressed by the so-called **p-value**: $$P(OtherRandomDifference>MyDifference|H_0)$$

So, if we obtain, for example, **p-value = 30%**, then we can say that **there is a 30% probability that another sample will show a greater deviation from $H_0$ than what we observed in our own data**. Based on this, we tend to accept $H_0$, because if we assume it to be true, then we would expect to see even greater deviations from $H_0$ in other random samples with a fairly high probability. Thus, **the deviation we observed is not large enough compared to $H_0$ (not significant), so we do not reject it, because our data does not provide enough evidence for such a decision**.  

However, if we obtain, for example, **p-value = 0.1%**, then we can say that **there is only a 0.1% probability that another sample will show a greater deviation from $H_0$ than what we observed in our own data**. Based on this, we tend to reject $H_0$, because in this case, assuming $H_0$ to be true, we would very rarely see a greater deviation from $H_0$ than what we actually observed. Therefore, **the deviation we observed compared to $H_0$ is large enough (significant), so we confidently reject it, as our data provides sufficient evidence for this decision**.  

Based on all this, the **decision rule** between $H_0$ and $H_1$ is as follows.

If the **p-value is too high, we accept** $H_0$, because we do not have enough evidence from the observed sample for rejecting it. Conversely, if the **p-value is too low, we reject** $H_0$ and **accept** $H_1$. 

The question is: how do we decide whether a p-value is too high or too low? For this, we use the so-called **significance level, denoted by** $\alpha$. The significance level represents the **maximum accepted probability of error when rejecting $H_0$**.  

So, if we say that $\alpha = 5\%$, then for a p-value below $5\%$, we **reject $H_0$, because the observed data shows a large enough deviation from $H_0$ that we can ensure a maximum error probability of $5\%$.** On the other hand, if our p-value is above $5\%$, we **accept $H_0$, because the observed data does NOT show a large enough deviation from $H_0$ to justify rejection with at most a $5\%$ error probability.**  

At this point, we might **believe the following**:  

- **p-value** $> \alpha \rightarrow H_0$  
- **p-value** $\leq \alpha \rightarrow H_1$ 

But of course, **LIFE IS NEVER THAT SIMPLE!!** The main problem here is that **our decision could heavily depend on what exact value we set for $\alpha$!** For example, say we set our maximum allowed error probability to $\alpha = 5\%$. If we then obtain a p-value of $4.8\%$, we would reject $H_0$ under $\alpha = 5\%$, but under $\alpha = 3\%$, we would accept $H_0$.<br>
Because of this, we typically apply a **range of common significance levels, which fall between $1\%$ and $10\%$**. We do not want to allow an error probability above $10\%$, as that would be unreasonably high. On the other hand, an error probability of **$0\%$ is only possible if we observe the entire population and know the true population value of the examined statistical parameter**. At that point, however, we lose the whole point of sampling — namely, that we do not have to observe every past and future data point to determine the true population value of our statistic. This $ \alpha = 0\%$ case is similar to why a $100\%$ confidence interval is meaningless — because it would say that based on our sample, the parameter could be anywhere between $\pm \infty$... :)<br>
Furthermore, we will later see from **simulations that both true and false $H_0$ statements can easily produce p-values between $1\%$ and $10\%$ in reality!**  

So, based on this long-winded explanation, a more reasonable solution seems to be to **not make a decision between $H_0$ and $H_1$ when the p-value is between $1\%$ and $10\%$, because the decision would be too sensitive to the exact choice of $\alpha$**.<br>
If our sample data produces a p-value in the range $1\%$ to $10\%$, the best thing to do is **increase the sample size ($n$) until the p-value clearly falls below $1\%$ or rises above $10\%$**. This method is called <a href="https://en.wikipedia.org/wiki/Sequential_analysis" target="_blank">sequential analysis</a>, and it is widely used in clinical drug trials and industrial quality control. The method was originally developed by a Hungarian statistician, <a href="https://en.wikipedia.org/wiki/Abraham_Wald" target="_blank">Abraham Wald</a>, for the U.S. Air Force during World War II (since he was Jewish, he was not "needed" in Hungary at the time). So, like everything important in the world, this too was invented by Hungarians. :)  

Thus, the **more correct decision rules in hypothesis testing** are:  

- **p-value** $>10\% \rightarrow H_0$
- $1\% <$ **p-value** $\leq 10\% \rightarrow$ **no decision**
- **p-value** $\leq1\% \rightarrow H_1$ 

Another headache is that we must be very cautious with p-values between $1\%$ and $10\%$, because **p-values and $\alpha$ only describe one type of decision error in hypothesis testing!**  The **p-value and $\alpha$ only provide information about the so-called TYPE I ERROR: the probability of rejecting a true $H_0$.**<br>
BUT we also have a **TYPE II ERROR**, which is **the probability of mistakenly accepting a false $H_0$.** The bad news is that **we cannot estimate or control the probability of Type II error based on our observed sample!**<br>
This is the major downside of hypothesis testing: if **we accept $H_0$, we do not actually know how likely we are to be wrong—we only know the maximum probability of rejecting a true $H_0$, because that is the significance level ($\alpha$).**<br>
For this reason, many statisticians phrase it carefully, saying that we do not "accept" $H_0$, but rather, **we "fail to reject" $H_0$.**  

For a better understanding of the problem, I think the following meme will be of great help. :)

<center>
![](Type2.png){width=40%}
</center>

<br>Now, let’s see **how to calculate the p-value based on an observed sample when the statistical parameter of interest is the mean**, i.e., $\mu$!

## 2. The *t-test* for the Population Mean

The calculation of the p-value **always requires a statistical indicator called a test statistic, which we can always compute solely from the observed sample data**. Then, **the p-value is calculated from the test statistic based on a probability distribution (standard normal, t-distribution, chi-square, etc.)**.

Let us denote the **HYPOTHETICAL value of the population mean** as $\mu_0$. This is the value that I **assume** in the statement about the true population mean. In Section 1, this was $150$ minutes. With this notation, the test statistic for the mean appears as follows: $$\frac{\bar{y}-\mu_0}{\frac{s}{\sqrt{n}}}$$

Thus, our test statistic is **simply the difference between the sample mean and the hypothetical mean, divided by the standard error of the mean**.

If our reasoning in $H_0$ is correct, and **we truly managed to take the real population mean $\mu$ as the theoretical** (hypothetical) **mean $\mu_0$, then with many repeated samples, our test statistic follows a t-distribution with $n-1$ degrees of freedom**: $$\frac{\bar{y}-\mu}{\frac{s}{\sqrt{n}}} \sim t(n-1)$$

The principle behind this formula is that **we apply the technical null hypothesis $\mu = \mu_0$ ($H_0^T$) in all six different types of $H_0$ and $H_1$ pairs, so we can determine the distribution of the test statistic across many samples**.

We can quickly verify this by **drawing $10000$ samples of size $n=100$ from the population of swimmers crossing Lake Balaton**, as we did in <a href="Chapter05.html" target="_blank">Chapter 5</a>.<br>  
Now, we **reload the Excel table containing $10000$ samples into a data frame**, which we created in <a href="Chapter04.html" target="_blank">Chapter 4</a>. This Excel file is available <a href="https://github.com/KoLa992/Statistical-Modelling-Lecture-Notes/blob/main/SwimmingSamples.xlsx" target="_blank">here</a>.

```{r}
samples_100 <- as.data.frame(read_excel("SwimmingSamples.xlsx"))
rownames(samples_100) <- paste0("Sample",1:10000) # indicate in the rownames that each row is one sample
head(samples_100)
```

Okay, from the results, we see that the data frame is structured such that **each row contains one sample of 100 elements, and the sample elements** (i.e., the recorded time results in minutes for each sampled swimmer) **are stored in columns**.

We compute the sample mean time and corrected standard deviation for each sample. Be sure that the second parameter of the `sapply` function is `1`, so that we take means and standard deviations row-wise instead of column-wise. Also, ensure that the functions are applied only to the first $100$ columns, as those contain the actual sample elements. Basically, do everything exactly as we did in <a href="Chapter05.html" target="_blank">Section 2 of Chapter 5</a>.

```{r}
samples_100$sample_mean <- apply(samples_100[,1:100], 1, mean)
samples_100$sample_sd <- apply(samples_100[,1:100], 1, sd) # corrected -> unbiased

head(samples_100[,97:102])
```

Then, we can **compute our test statistic for two pairs of $H_0$ and $H_1$, where we know that $H_0$ is true in one case and false in another**:

- Case of **TRUE** $H_0$: $H_0:\mu=167$ and $H_1: \mu < 167$
- Case of **FALSE** $H_0$: $H_0^T:\mu=150$ and $H_1: \mu > 150$

```{r}
mu_0_true <- mean(swimming_population$TIME) # true population mean
mu_0_false <- 150 # arbitrary value
n <- 100 # sample size

samples_100$test_stat_H0true <- (samples_100$sample_mean - mu_0_true)/(samples_100$sample_sd/sqrt(n))
samples_100$test_stat_H0false <- (samples_100$sample_mean - mu_0_false)/(samples_100$sample_sd/sqrt(n))

head(samples_100[,99:104])
```

Alright! Now that we have the test statistics for both scenarios, **let's examine how the histograms of these test statistics from $10000$ samples compare to the density function of the $t(n-1)$, i.e., $t(100-1)$, distribution**.<br>  
The code for visualizing the fit of the density function to the histogram follows the exact logic of the code found in <a href="Chapter03.html" target="_blank">Section 1.3 of Chapter 3</a>.

```{r warning=FALSE}
library(ggplot2)

# general ggplot function without defining any axes
ggplot(samples_100) +
  # histogram of test statistic when H0 is TRUE (x axis)
  geom_histogram(aes(x = test_stat_H0true, y = after_stat(density), fill="H0 TRUE")) +
  # histogram of test statistic when H0 is FALSE (x axis)
  geom_histogram(aes(x = test_stat_H0false, y = after_stat(density), fill="H0 FALSE")) +
  # adding the density function of the t(n-1) distribution
  stat_function(fun = dt, 
                args = list(df = (n-1)),
                col = 'blue', linewidth = 1)
```

Great! So **when $H_0$ is true, the test statistic does indeed follow a t-distribution in our repeated sampling experiment**. When $H_0$ is false, the histogram forms some other symmetric distribution that is most definately NOT a t-distribution. However, we are not particularly concerned with what this distribution is. :)

From the fact that the test statistic follows a t-distribution under repeated sampling when $H_0$ is true, **we can compute the p-value**. Essentially, we calculate areas under the *t-distribution* density function. To do this, we must consider **what the best-case scenario for $H_0$ is regarding the test statistic**:

- **Two-tailed** tests ($H_0: \mu=\mu_0$ and $H_1: \mu \neq \mu_0$): If the **test statistic is exactly** $0 \rightarrow$ this theoretical case suggests that the hypothetical mean and the true population mean are identical, so with probability $1$, I would obtain greater deviations from $H_0$ in other samples.
- **Left-tailed** tests ($H_0: \mu\geq\mu_0$ and $H_1: \mu < \mu_0$): If the **test statistic =** $+\infty \rightarrow$ this theoretical case suggests that the population mean ($\mu$) is exactly $+\infty$, making every $\mu_0$ smaller, so with probability $1$, I would obtain greater deviations from $H_0$ than what's observed.
- **Right-tailed** tests ($H_0: \mu=\mu_0$ and $H_1: \mu > \mu_0$): If the **test statistic =** $-\infty \rightarrow$ this theoretical case suggests that the population mean ($\mu$) is exactly $-\infty$, making every $\mu_0$ larger, so with probability $1$, I would obtain greater deviations from $H_0$ than what's observed.

Based on this, if **we have a known test statistic value denoted as $t$, then the p-value is calculated from the $t(n-1)$ distribution as shown in the following figure**:

<center>
![](pval_fromt.jpg){width=90%}
</center>

<br>As seen, **in all three cases, the p-value is computed such that the further the specific test statistic $t$ moves away from the best case for $H_0$, the smaller the p-value becomes $\rightarrow$ it becomes less and less likely to obtain greater deviations from $H_0$ than observed!!**

Based on all this, let's **calculate the p-value** using the `pt` function **for all $10000$ samples for the two previously examined pairs of $H_0$ and $H_1$, where $H_0$ was true in one case and false in the other**:

- **TRUE** $H_0$ case: $H_0:\mu=167$ and $H_1: \mu < 167$
- **FALSE** $H_0$ case: $H_0:\mu=150$ and $H_1: \mu > 150$

Note that the `1-` part before the `pt` function is included for the false $H_0$ case because it is a **right-tailed test**.

```{r}
samples_100$p_value_H0true <- pt(samples_100$test_stat_H0true, df = (n-1))
samples_100$p_value_H0false <- 1-pt(samples_100$test_stat_H0false, df = (n-1))

head(samples_100[,101:106])
```

Great! Based on the first six samples, the expected p-value results were obtained: high values when $H_0$ is true, low values when $H_0$ is false. To interpret more precisely, let's examine the **4th sample**:

- **When $H_0$ is true, there is a 69.78% probability of obtaining greater deviations from $H_0$ than observed in sample 4**.
- **When $H_0$ is false, there is only a 0.01% probability of obtaining greater deviations from $H_0$ than observed in sample 4**.

At the same time, in sample $1.$, we see that the p-value is only $3.5\%$ when $H_0$ is true. So, the rule **"we do not make a decision if the p-value is between $1\%$ and $10\%$" seems quite reasonable**! :)

Similarly, the caution taken for p-values in the range of $1\%$ to $10\%$ is reinforced when we examine the p-values for true and false $H_0$ using a box plot.

```{r}
boxplot(samples_100[,c("p_value_H0true", "p_value_H0false")])
```

As expected, when $H_0$ is true, the middle $50\%$ of p-values, that is, the interquartile range ($IQR$), is significantly higher than in the case of false $H_0$. However, it is also very important to observe in the plot that **large p-values are considered outliers when $H_0$ is false, but small p-values are not outliers when $H_0$ is true!** Thus, sometimes it may be worth using a stricter $\alpha$ than $1\%$. Many people reject $H_0$ only if the p-value is smaller than $0.1\%=0.001$, meaning they work with $\alpha=0.001$.

Based on our results, we can calculate the proportion of decision errors when $\alpha=5\%$ and $H_0$ is true. That is, the **empirical probability of a Type I error**: how often did we reject a true $H_0$? According to the definition of the significance level, this proportion should be around $5\%$.

```{r}
mean(samples_100$p_value_H0true<0.05)
```

Well, the result is $6.05\%$, which is 1 percentage point higher than $5\%$, but not by much. It looks like the system is working properly. If we examined even more samples than 10000, this proportion would get closer and closer to $5\%$.

Moreover, since in this simulation case, we know when $H_0$ is true or false, we can now also calculate the **empirical probability of a Type II error** at $\alpha=5\%$: the proportion of times we accept a false $H_0$.

```{r}
mean(samples_100$p_value_H0false>=0.05)
```

Not bad, $0.52\%$!

Let us observe the completely logical relationship that if the **probability of a Type I error (significance level) decreases, then the probability of a Type II error increases**. Now, let's calculate the two empirical error probabilities at $\alpha=1\%$. That is, we lowered the expected probability of a Type I error to $1\%$.

```{r}
mean(samples_100$p_value_H0true<0.01) # empirical probability of Type I error
mean(samples_100$p_value_H0false>=0.01) # empirical probability of Type II error
```

### 2.1. The *t-test* with a bulit-in R function

Luckily, we have an advantage because the **p-value of the t-test for the mean can be nicely computed using a built-in R function**!

For example, let’s take the $n=100$ sample taken at the beginning of Section 1, which we stored in the R object `swimming_sample`, which is a $100\times3$ data frame.

```{r}
str(swimming_sample)
```

Examine the following $H_0$ + $H_1$ pair for this specific sample:

- $H_0^T:\mu=150$
- $H_1 > 150$

Since the $H_1$ hypothesis contains the $>$ relational sign, the hypothesis test will be **right-tailed**.<br>
We know that here **$H_1$ is true, as we have previously seen that the average swim time in the entire population is $167.5$ minutes**. Therefore, we expect **a very low p-value**!

This result can be nicely obtained manually! First, we calculate the **test statistic** $\frac{\bar{y}-\mu_0}{\frac{s}{\sqrt{n}}}$ assuming $\mu_0=150$, since in hypothesis testing we examine whether the true population mean is significantly greater than this assumed (hypothetical) $150$-minute mean.

```{r}
hypothetical_mean <- 150
sample_mean <- mean(swimming_sample$TIME)
s <- sd(swimming_sample$TIME)
n <- nrow(swimming_sample)

test_stat <- (sample_mean - hypothetical_mean) / (s/sqrt(n))
test_stat
```

Great! From this, we can quickly obtain the p-value from the $t(n-1)$ distribution using the `pt` function. Here, we are calculating **the probability of falling above the test statistic, since this is a right-tailed test**.

```{r}
p_val_t_distr = 1 - pt(test_stat, df = n-1)
p_val_t_distr * 100 # in percentage format
```

**Based on the p-value, we can state that for the examined sample of size $100$, there is only a $0.203\%$ probability that another random sample would show a greater deviation from $H_0$ than what we observed here**. This **is even smaller than the smallest common significance level of $\alpha=1\%$, so we make the decision to reject $H_0$**.<br>
Based on this, $H_1$ is acceptable, meaning that the **average swim time in the population is significantly** (beyond sampling error) **greater than $150$ minutes**.<br>
Now we know that **we made the correct decision**, as we have seen that the population average swim time is $167.5$ minutes.

Fortunately, all of this can also be handled using the `t.test` function. If the function is provided with:

- the column containing the observations of our current sample from a data frame as its first parameter,
- the theoretical mean, i.e., $\mu_0$, in the `mu` parameter,
- the test direction in the `alternative` parameter based on the relational sign in $H_1$
  * possible values: `'two.sided'`, `'less'`, `'greater'`, corresponding to the relational signs $\neq$, $<$, and $>$ in $H_1$

Then the **function provides the p-value of the t-test and the test statistic as well**.

```{r}
t.test(swimming_sample$TIME, mu = 150, alternative = "greater")
```

Awesome, we get the same test statistic ($2.9416$) and the p-value of $0.2033\%$! :)

### 2.2. Assumptions of the *t-test*

Of course, this t-test-based hypothesis test also has prerequisites, just as different confidence interval formulas do. **If these conditions are not met, then no matter what the p-value is, it cannot be used to make a decision about $H_0$ and $H_1$**, because the underlying assumptions of the applied formulas are NOT met by the data.

The t-test essentially has **two conditions** to be considered:

1. If we have a **small sample** ($n<100$), we assume that **the data from which the sample was drawn follows a normal distribution**.
2. If the **sample size is large** ($n\geq100$), then **no additional assumptions need to be considered**.

Now, our sample of $n=100$ from the population of Balaton swimmers is already a large sample, since $100\geq100$, but let's still check the **distribution of the time result data based on the sample**.

```{r}
hist(swimming_sample$TIME)
```

Hmm, although this does not matter due to the large sample size, it is reassuring that the distribution of crossing times does not deviate drastically from the density function of a normal distribution. It only has a very slight right-skewed tendency, but nothing alarming. :)

## 3. The *z-test* for the Population Mean

Of course, in the case of confidence intervals for the mean, we observed that **for a large degree of freedom ($df \geq 100$), the t-distribution is essentially the same as the standard normal distribution**.

That is, to put it a bit more mathematically: **if $n \geq 100$, then under the true null hypothesis ($H_0$)** (i.e., when we substitute the true population mean: $\mu$, for $\mu_0$), our **test statistic follows a standard normal ($N(0,1)$) distribution**: $$\frac{\bar{y}-\mu}{\frac{s}{\sqrt{n}}} \sim N(0,1)$$

Based on this, we can **compute the p-value from the test statistic using exactly the same principle we applied in Section 2 for the t-distribution**. Since the null and alternative hypotheses remain unchanged ($H_0^T: \mu = 150$ and $H_1 > 150$), we still have a right-tailed test, and since the sample elements remain the same in our 100-element sample, the **test statistic is also the same as the one used in Section 2.1**.<br>
**Since this remains a right-tailed test**, we must **determine the probability of falling above the test statistic in the standard normal distribution** using the `pnorm` function with the $1-$ version.

```{r}
p_val_norm_distr <- 1 - pnorm(test_stat)
p_val_norm_distr * 100 # percentage format

p_val_t_distr * 100 # p-value from t-distribution in percentage format
```

Both p-values are around $0.2\%$, so there really is no noticeable difference between the standard normal and t-distribution when calculating the p-value.

## 4. The *z-test* for the Population Proportion

Of course, we can conduct hypothesis tests not only for **population means ($\mu$)** but also for **population proportions ($P$)**. We can then compute p-values for these and decide whether our original hypothesis $H_0$ or the alternative hypothesis $H_1$ is considered true at a given significance level ($\alpha$).

Let’s see if **the proportion of people who swim across Lake Balaton in more than 3 hours can be considered 30\%**. Mathematically, this claim or statement means that the population proportion of swimmers taking more than 3 hours = 180 minutes is exactly 30%: $P = 0.3$ Since the **claim allows for equality, it is placed in $H_0$**. Accordingly, **$H_1$ negates the original claim, and we "support" $H_0$ in the hypothesis test**. Thus, our $H_0$ and $H_1$ hypotheses are structured as follows:

- $H_0:P=0.3$
- $H_1:P\neq0.3$
- Statement is in $H_0$


From our $H_0$ and $H_1$ hypotheses, it follows that the **theoretical proportion ($P_0$) against which we are testing is precisely $0.3$!!**

Now, based on <a href="Chapter08.html" target="_blank">Section 2 of Chapter 8</a>, we would create a column of $0-1$ values in our observed sample, where $1$ represents swimmers who took more than 3 hours (favorable observations for proportion calculations), while $0$ represents the others (unfavorable observations). Then, we conduct a hypothesis test for the mean of this column (since $n=100 \geq 100$, this can simply be a z-test).

However, **there is a small issue with this approach!** Specifically, the **test statistic for a proportion hypothesis test is structured as follows**: $$\frac{p-P_0}{\sqrt{\frac{P_0(1-P_0)}{n}}} \sim N(0,1)$$

The **"small issue"** lies in the denominator, i.e., the **standard error**. The formula essentially states that we take the difference between the sample and theoretical proportions ($p-P_0$) divided by the standard error of the proportion. This follows the same principle as the hypothesis test for the mean, as seen in Section 2. BUT! In the **standard error formula, the theoretical proportion ($P_0$) is used** instead of the sample proportion ($p$). However, if we calculated the test statistic as a hypothesis test for the mean of a $0-1$ column, we would use $\sqrt{\frac{p(1-p)}{n}}$ instead of $\sqrt{\frac{P_0(1-P_0)}{n}}$ in the denominator. This means that we must manually compute the test statistic ourselves. The p-value is then obtained from the standard normal ($N(0,1)$) distribution, just as we saw for means in Sections 2 and 3. This is why this hypothesis test is also called a *z-test*.

To perform the test, we first need to compute the total sample size ($n$) and the number of favorable observations ($k$), which in this case refers to the number of swimmers who took more than 180 minutes. From these, we can even calculate the sample proportion ($p$).

```{r}
# sample size
n <- nrow(swimming_sample)

# favourable cases for the proportion
fav <- sum(swimming_sample$TIME > 180)

# sample proportion
p <- fav/n
p
```

From this, we see that the proportion of swimmers who took more than 180 minutes in the sample is exactly $27\%$, which is not equal to $0.3 = 30\%$, so **$H_1$ appears to be true**. However, **this could easily be due to sampling error**, since we are working with only a 100-element sample, **not the entire population of swimmers**. That is exactly why **we need the p-value**, which tells us how likely it is that the observed sample deviation ($0.27 \neq 0.30$) is a significant difference between the sample proportion and the theoretical proportion.

Now, we compute the test statistic using the given formula and then determine the p-value according to the left-tailed test rules based on the standard normal distribution.

```{r}
# hypothetical proportion
P_0 <- 0.3

test_stat_p <- (p-P_0)/(sqrt(P_0*(1-P_0)/n))

p_val_for_prop <- 2*pnorm(test_stat_p) # two-tailed test -> 2*
p_val_for_prop
```

The resulting p-value is high, at $51.2\%$, meaning there is a $51.2\%$ chance of obtaining a greater deviation from $H_0$ than what we observed. This is a very high probability, higher than even the most common significance level ($10\%$), so **we conclude that we cannot reject $H_0$**. **The true population proportion of swimmers taking more than 180 minutes can indeed be considered $30\%$**; the difference between the sample proportion and $0.3$ is **NOT significant**.

Since this is a z-test and the p-value is computed from the standard normal distribution, the test requires that the sample size is sufficiently large. Specifically, both $n \times P_0$ and $n \times (1 - P_0)$ must be greater than $10$. That is, the **number of favorable and unfavorable observations in the sample must each be at least $10$ under the true null hypothesis**.

Let’s check if these assumptions are met.

```{r}
n*P_0
n*(1-P_0)
```

Since both $30$ and $70$ are greater than $10$, so we are good to proceed! :)

## 5. The General Procces of Hypothesis Testing

From all the above, we can conclude that **regardless of the statistical parameter** (mean, proportion, etc.), **every hypothesis test follows four key steps**:

1. Formulating the **null and alternative hypotheses** ($H_0$ and $H_1$) from the initial claim.
2. Computing the **test statistic** from the observed $n$-element sample data.
3. Calculating the **p-value** based on the test statistic and a probability distribution.
4. Making a **decision** based on the p-value: determining whether $H_0$ or $H_1$ is likely to be true for the entire population.

These $4$ steps remain the **same in every hypothesis test**, and **Steps $1$ and $4$ always follow the same principles as we've seen so far**; they do not change. No matter what happens, the formulation of $H_0$ and $H_1$ and the decision between them based on the *p-value* follow fixed rules. What **can differ in specific cases are Steps $2$ and $3$**, where we compute the test statistics and p-values. However, **there is almost always a built-in R function for these calculations; we just need to know how to configure the parameters of these functions and understand the specific test assumptions properly**.